{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Tuple, List, Dict, Any, Optional, Union\n",
    "import ijson\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findClosestPoint(\n",
    "    point: Tuple[float, float, float], pointsStr: str\n",
    ") -> Tuple[int, int]:\n",
    "    # point is a tuple of (x, y, z)\n",
    "    # pointsStr is a string of a list of points seperated by @\n",
    "    stringPoints = pointsStr.split(\"@\")\n",
    "    point = [float(value) for value in point]\n",
    "    # cast each strring from 0.1,0.2,0.3 to [0.1, 0.2, 0.3]\n",
    "    points = [list(map(float, point.split(\",\"))) for point in stringPoints]\n",
    "    # find closest match of point in points\n",
    "    closest = min(points, key=lambda p: sum((a - b) ** 2 for a, b in zip(p, point)))\n",
    "    # return index of closest point and total length of points\n",
    "    return (points.index(closest), len(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distanceToLastRoadPoint(\n",
    "        point: Tuple[float, float, float], pointsStr: str\n",
    ") -> float:\n",
    "    stringPoints = pointsStr.split(\"@\")\n",
    "    point = [float(value) for value in point]\n",
    "    points = [list(map(float, point.split(\",\"))) for point in stringPoints]\n",
    "    lastPoint = points[-1]\n",
    "    # find index in last point closest to 0.56\n",
    "    # round to 2 decomal places\n",
    "    if round(lastPoint[1], 1) == 0.5:\n",
    "        lastPoint[1], lastPoint[2] = lastPoint[2], lastPoint[1]\n",
    "\n",
    "    return sum((a - b) ** 2 for a, b in zip(lastPoint, point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road1 = []\n",
    "road2 = []\n",
    "road3 = []\n",
    "road4 = []\n",
    "road5 = []\n",
    "road6 = []\n",
    "road7 = []\n",
    "road8 = []\n",
    "road9 = []\n",
    "\n",
    "def first_derivative(x):\n",
    "                    return [abs(x[i+1] - x[i]) for i in range(len(x)-1)]\n",
    "\n",
    "\n",
    "def loadJsons(filenames):\n",
    "    results: Dict[str, Dict[int, Any]] = {}\n",
    "    roads: Dict[str, Any] = {}\n",
    "    road_success_dict = {}\n",
    "\n",
    "    df_dict = {}\n",
    "    for filename in filenames:\n",
    "        # iterate over each object in the json file\n",
    "        with open(filename, \"r\") as f:\n",
    "            objects = ijson.items(f, \"item\")\n",
    "            for benchmarking_obj in objects:\n",
    "                # access the scenario\n",
    "                scenario = benchmarking_obj[\"scenario\"]\n",
    "                # acess the waypoints within scenario\n",
    "                waypoints = scenario[\"waypoints\"]\n",
    "                # get a hash of the waypoints\n",
    "                waypointsHash = hashlib.sha256(str(waypoints).encode()).hexdigest()\n",
    "                # if the hash equial asjdn write the object in a list\n",
    "                # if waypointsHash == \"44160852c628732f811ea9907657f37de4d02894da958e5d858d669ff2578995\":\n",
    "                #    road1.append(benchmarking_obj)\n",
    "                # elif waypointsHash == \"be859fbf2b9946c8d4834d026e413df51dc9f70bd637bcd0dc45ec7e8e3926af\":\n",
    "                #    road2.append(benchmarking_obj)\n",
    "                # elif waypointsHash == \"0a246a82022e55cbd10c2d46db0e90f41111798ccb7179b3c9ef1f9d7e549882\":\n",
    "                #    road3.append(benchmarking_obj)\n",
    "                # elif waypointsHash == \"493f3da92ee367c09bf8647aab2fb671462701ef0397c3749dd93b18585e1aa1\":\n",
    "                #    road4.append(benchmarking_obj)\n",
    "                # elif waypointsHash == \"382e50f80e16e79861d84d5d102dae4b793c449941b48d4963560d13e4b0f053\":\n",
    "                #    road5.append(benchmarking_obj)\n",
    "                # elif waypointsHash == \"90a8e61085e4e9918cb2c35f1d751b76a3a4b9ba5ef677deaa441487827fd026\":\n",
    "                #    road8.append(benchmarking_obj)\n",
    "\n",
    "                roads[waypointsHash] = waypoints\n",
    "\n",
    "                # get the perturbation_function\n",
    "                perturbation_function = scenario[\"perturbation_function\"]\n",
    "                # get the perturbation_scale\n",
    "                perturbation_scale = scenario[\"perturbation_scale\"]\n",
    "                # get the boolean value for isSuccess\n",
    "                isSuccess = benchmarking_obj[\"isSuccess\"]\n",
    "                # get the boolean value for isSuccess\n",
    "                timeout = benchmarking_obj[\"timeout\"]\n",
    "\n",
    "                # get the average value of the float list in xte\n",
    "                xte = benchmarking_obj[\"xte\"]\n",
    "                xteAvg = sum([abs(i) for i in xte]) / len(xte)\n",
    "                xteAvg = float(xteAvg)\n",
    "                max_xte = max([abs(i) for i in xte])\n",
    "                max_xte = float(max_xte)\n",
    "\n",
    "                derivative = first_derivative(benchmarking_obj[\"xte\"])\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "                # get the last value in the pos list\n",
    "                pos = benchmarking_obj[\"pos\"]\n",
    "                print(pos[-1])\n",
    "                posLast = pos[-1]\n",
    "                # swap the y and z values\n",
    "                posLast[1], posLast[2] = posLast[2], posLast[1]\n",
    "\n",
    "                # find the closest point to the last value in pos\n",
    "                closestPoint, totalPoints = findClosestPoint(posLast, waypoints)\n",
    "\n",
    "                quickness = (closestPoint / totalPoints)\n",
    "\n",
    "                # get distance to last road point\n",
    "                distance = distanceToLastRoadPoint(posLast, waypoints)\n",
    "\n",
    "                isSuccess = isSuccess\n",
    "\n",
    "                time=len(xte)/20\n",
    "\n",
    "                # check if there is a key for the perturbation_function\n",
    "                if perturbation_function not in results:\n",
    "                    results[perturbation_function] = {}\n",
    "                # check if there is a key for the perturbation_scale\n",
    "                if perturbation_scale not in results[perturbation_function]:\n",
    "                    results[perturbation_function][perturbation_scale] = {\n",
    "                        \"success\": [],\n",
    "                        \"xte\": [],\n",
    "                        \"quickness\": [],\n",
    "                        \"roadHash\": [],\n",
    "                        \"time\": [],\n",
    "                        \"timeout\": [],\n",
    "                        \"maxxte\": [],\n",
    "                        \"derivative\": [],\n",
    "                        \"max_derivative\": [],\n",
    "                    }\n",
    "                # increment the success or failure\n",
    "                results[perturbation_function][perturbation_scale][\"success\"].append(\n",
    "                    isSuccess\n",
    "                )\n",
    "\n",
    "                # append the xte and quickness values\n",
    "                results[perturbation_function][perturbation_scale][\"xte\"].append(xteAvg)\n",
    "                results[perturbation_function][perturbation_scale][\"maxxte\"].append(max_xte)\n",
    "                results[perturbation_function][perturbation_scale][\"quickness\"].append(\n",
    "                    quickness\n",
    "                )\n",
    "                results[perturbation_function][perturbation_scale][\"time\"].append(\n",
    "                    time\n",
    "                )\n",
    "                results[perturbation_function][perturbation_scale][\"roadHash\"].append(\n",
    "                    waypointsHash\n",
    "                )\n",
    "                results[perturbation_function][perturbation_scale][\"timeout\"].append(\n",
    "                    timeout\n",
    "                )\n",
    "                results[perturbation_function][perturbation_scale][\"derivative\"].append(\n",
    "                    derivative\n",
    "                )\n",
    "\n",
    "                results[perturbation_function][perturbation_scale][\"max_derivative\"].append(\n",
    "                    max(derivative)\n",
    "                )\n",
    "\n",
    "                # other index\n",
    "                index = f\"Scale_{perturbation_scale}_Success\"\n",
    "                index2 = f\"Scale_{perturbation_scale}_XTE\"\n",
    "                index3 = f\"Scale_{perturbation_scale}_Quickness\"\n",
    "                index4 = f\"Scale_{perturbation_scale}_RoadHash\"\n",
    "                index5 = f\"Scale_{perturbation_scale}_MaxXTE\"\n",
    "                index6 = f\"Scale_{perturbation_scale}_Distance\"\n",
    "                index7 = f\"Scale_{perturbation_scale}_Time\"\n",
    "                index8 = f\"Scale_{perturbation_scale}_Timeout\"\n",
    "                index9 = f\"Scale_{perturbation_scale}_XTEderivative\"\n",
    "\n",
    "\n",
    "                if perturbation_function not in df_dict:\n",
    "                    df_dict[perturbation_function] = {}\n",
    "                if index not in df_dict[perturbation_function]:\n",
    "                    df_dict[perturbation_function][index] = [isSuccess]\n",
    "                elif (len(df_dict[perturbation_function][index])<10):\n",
    "                    df_dict[perturbation_function][index].append(isSuccess)\n",
    "                    print(len(df_dict[perturbation_function][index]))\n",
    "                if index2 not in df_dict[perturbation_function]:\n",
    "                    df_dict[perturbation_function][index2] = [xteAvg]\n",
    "                elif (len(df_dict[perturbation_function][index2])<=10):\n",
    "                    df_dict[perturbation_function][index2].append(xteAvg)\n",
    "                if index3 not in df_dict[perturbation_function]:\n",
    "                    df_dict[perturbation_function][index3] = [quickness]\n",
    "                elif (len(df_dict[perturbation_function][index3])<=10):\n",
    "                    df_dict[perturbation_function][index3].append(quickness)\n",
    "                if index4 not in df_dict[perturbation_function]:\n",
    "                    df_dict[perturbation_function][index4] = [waypointsHash]\n",
    "                elif (len(df_dict[perturbation_function][index4])<=10):\n",
    "                    df_dict[perturbation_function][index4].append(waypointsHash)\n",
    "                if index5 not in df_dict[perturbation_function]:\n",
    "                    df_dict[perturbation_function][index5] = [max_xte]\n",
    "                elif (len(df_dict[perturbation_function][index5])<=10):\n",
    "                    df_dict[perturbation_function][index5].append(max_xte)\n",
    "                if index6 not in df_dict[perturbation_function]:\n",
    "                    df_dict[perturbation_function][index6] = [distance]\n",
    "                elif (len(df_dict[perturbation_function][index6])<=10):\n",
    "                    df_dict[perturbation_function][index6].append(distance)\n",
    "                if index7 not in df_dict[perturbation_function]:\n",
    "                    df_dict[perturbation_function][index7] = [time]\n",
    "                elif (len(df_dict[perturbation_function][index7])<=10):\n",
    "                    df_dict[perturbation_function][index7].append(time)\n",
    "                if index8 not in df_dict[perturbation_function]:\n",
    "                    df_dict[perturbation_function][index8] = [timeout]\n",
    "                elif (len(df_dict[perturbation_function][index8])<=10):\n",
    "                    df_dict[perturbation_function][index8].append(timeout)\n",
    "                if index9 not in df_dict[perturbation_function]:\n",
    "                    df_dict[perturbation_function][index9] = [derivative]\n",
    "                elif (len(df_dict[perturbation_function][index9])<=10):\n",
    "                    df_dict[perturbation_function][index9].append(derivative)\n",
    "\n",
    "                # check if there is a key for roadHash in roads\n",
    "                if waypointsHash not in roads:\n",
    "                    roads[waypointsHash] = waypoints\n",
    "                if waypointsHash not in road_success_dict:\n",
    "                    road_success_dict[\n",
    "                        waypointsHash\n",
    "                    ] = f\"{perturbation_function}_{perturbation_scale}-\"\n",
    "                if isSuccess:\n",
    "                    road_success_dict[\n",
    "                        waypointsHash\n",
    "                    ] += f\"{perturbation_function}_{perturbation_scale}-\"\n",
    "\n",
    "\n",
    "    return results, roads, road_success_dict, df_dict\n",
    "\n",
    "# type=\"donkey\"\n",
    "type=\"udacity\"\n",
    "\n",
    "if type==\"donkey\":\n",
    "    results, roads, road_success_dict, df_dict = loadJsons([\"donkey_benchmark_normal_perturbations_dave/donkey_benchmark_normal_perturbations_road1_dave.json\",\"donkey_benchmark_normal_perturbations_dave/donkey_benchmark_normal_perturbations_road2_dave.json\",\"donkey_benchmark_normal_perturbations_dave/donkey_benchmark_normal_perturbations_road3_dave.json\",\"donkey_benchmark_normal_perturbations_dave/donkey_benchmark_normal_perturbations_road4_dave.json\",\"donkey_benchmark_normal_perturbations_dave/donkey_benchmark_normal_perturbations_road5_dave.json\",\"donkey_benchmark_normal_perturbations_dave/donkey_benchmark_normal_perturbations_road6_dave.json\",\"donkey_benchmark_normal_perturbations_dave/donkey_benchmark_normal_perturbations_road7_dave.json\",\"donkey_benchmark_normal_perturbations_dave/donkey_benchmark_normal_perturbations_road8_dave.json\",\"donkey_benchmark_normal_perturbations_dave/donkey_benchmark_normal_perturbations_road9_dave.json\",\"donkey_benchmark_normal_perturbations_dave/donkey_benchmark_normal_perturbations_road10_dave.json\"])\n",
    "    # results, roads, road_success_dict, df_dict = loadJsons(['donkey_benchmark_baseline_VANILLA_DAVE2.json'])\n",
    "\n",
    "else:\n",
    "    results, roads, road_success_dict, df_dict = loadJsons([\"Udacity_Benchmark/benchmark_udacity_normal_perturbations_road_0_dave2_udaciy_100k.json\",\"Udacity_Benchmark/benchmark_udacity_normal_perturbations_road_1_dave2_udaciy_100k.json\",\"Udacity_Benchmark/benchmark_udacity_normal_perturbations_road_2_dave2_udaciy_100k.json\",\"Udacity_Benchmark/benchmark_udacity_normal_perturbations_road_3_dave2_udaciy_100k.json\",\"Udacity_Benchmark/benchmark_udacity_normal_perturbations_road_4_dave2_udaciy_100k.json\",\"Udacity_Benchmark/benchmark_udacity_normal_perturbations_road_5_dave2_udaciy_100k.json\",\"Udacity_Benchmark/benchmark_udacity_normal_perturbations_road_6_dave2_udaciy_100k.json\",\"Udacity_Benchmark/benchmark_udacity_normal_perturbations_road_7_dave2_udaciy_100k.json\",\"Udacity_Benchmark/benchmark_udacity_normal_perturbations_road_8_dave2_udaciy_100k.json\",\"Udacity_Benchmark/benchmark_udacity_normal_perturbations_road_9_dave2_udaciy_100k.json\"])\n",
    "    # results, roads, road_success_dict, df_dict = loadJsons([\"benchmark_udacity_baseline_dave2_udaciy_100k.json\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# iterate over a dict\n",
    "for key, value in results.items():\n",
    "    # iterate over a dict\n",
    "    for key2, value2 in value.items():\n",
    "        # iterate over a dict\n",
    "        for key3, value3 in value2.items():\n",
    "            # print the key and value\n",
    "            print(key, key2, key3, value3)\n",
    "            print(len(value3))\n",
    "\n",
    "\n",
    "def delete_entries(json_file, field, values):\n",
    "    # Read the JSON file\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Delete the first and third entry from the list\n",
    "    data = data[1:3] + data[4:]\n",
    "\n",
    "    # Write the updated JSON data back to the file\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "def findEntries(json_file, values):\n",
    "    # Read the JSON file\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # iterate over the data\n",
    "    counter = 0\n",
    "    for entry in data:\n",
    "        counter += 1\n",
    "        # check if the entry has the field\n",
    "        if \"scenario\" in entry and entry[\"scenario\"][\"waypoints\"] in values:\n",
    "            print(\"counter has value in field\")\n",
    "\n",
    "    # Write the updated JSON data back to the file\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def findEntries(json_file, values):\n",
    "    # Read the JSON file\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # iterate over the data\n",
    "    counter = 0\n",
    "    for entry in data:\n",
    "        counter += 1\n",
    "        # check if the entry has the field\n",
    "        if \"scenario\" in entry and entry[\"scenario\"][\"waypoints\"] in values:\n",
    "            print(\"counter has value in field\")\n",
    "\n",
    "    # Write the updated JSON data back to the file\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(df_dict, orient=\"index\").round(2)\n",
    "\n",
    "# copy dataframe as backup\n",
    "df_copy = df.copy()\n",
    "\n",
    "scale_0_columns = [col for col in df.columns if \"Scale_0\" in col and not \"RoadHash\" in col]\n",
    "scale_1_columns = [col for col in df.columns if \"Scale_1\" in col and not \"RoadHash\" in col]\n",
    "scale_2_columns = [col for col in df.columns if \"Scale_2\" in col and not \"RoadHash\" in col]\n",
    "scale_3_columns = [col for col in df.columns if \"Scale_3\" in col and not \"RoadHash\" in col]\n",
    "scale_4_columns = [col for col in df.columns if \"Scale_4\" in col and not \"RoadHash\" in col]\n",
    "\n",
    "\n",
    "df.drop(\n",
    "    [\n",
    "        \"Scale_0_RoadHash\",\n",
    "        \"Scale_1_RoadHash\",\n",
    "        \"Scale_2_RoadHash\",\n",
    "        \"Scale_3_RoadHash\",\n",
    "        \"Scale_4_RoadHash\",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scale_0_Success</th>\n",
       "      <th>Scale_0_XTE</th>\n",
       "      <th>Scale_0_Quickness</th>\n",
       "      <th>Scale_0_MaxXTE</th>\n",
       "      <th>Scale_0_Distance</th>\n",
       "      <th>Scale_0_Time</th>\n",
       "      <th>Scale_0_Timeout</th>\n",
       "      <th>Scale_0_XTEderivative</th>\n",
       "      <th>Scale_1_Success</th>\n",
       "      <th>Scale_1_XTE</th>\n",
       "      <th>...</th>\n",
       "      <th>Scale_3_Timeout</th>\n",
       "      <th>Scale_3_XTEderivative</th>\n",
       "      <th>Scale_4_Success</th>\n",
       "      <th>Scale_4_XTE</th>\n",
       "      <th>Scale_4_Quickness</th>\n",
       "      <th>Scale_4_MaxXTE</th>\n",
       "      <th>Scale_4_Distance</th>\n",
       "      <th>Scale_4_Time</th>\n",
       "      <th>Scale_4_Timeout</th>\n",
       "      <th>Scale_4_XTEderivative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dynamic_snow_filter</th>\n",
       "      <td>10</td>\n",
       "      <td>1.99</td>\n",
       "      <td>100.00</td>\n",
       "      <td>2.96</td>\n",
       "      <td>[40.51412019000011, 35.34271095772501, 18.2336...</td>\n",
       "      <td>85.20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>10</td>\n",
       "      <td>2.02</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.18</td>\n",
       "      <td>6</td>\n",
       "      <td>2.36</td>\n",
       "      <td>79.85</td>\n",
       "      <td>3.81</td>\n",
       "      <td>[46.043610510000086, 21125.041126239987, 49.87...</td>\n",
       "      <td>116.92</td>\n",
       "      <td>2</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snow_filter</th>\n",
       "      <td>8</td>\n",
       "      <td>1.59</td>\n",
       "      <td>83.06</td>\n",
       "      <td>3.01</td>\n",
       "      <td>[39.080562370000024, 38.156865552874464, 41.45...</td>\n",
       "      <td>63.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5</td>\n",
       "      <td>1.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.86</td>\n",
       "      <td>76.94</td>\n",
       "      <td>4.08</td>\n",
       "      <td>[19264.174872740005, 7507.028343669046, 19641....</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>histogram_equalisation</th>\n",
       "      <td>7</td>\n",
       "      <td>1.93</td>\n",
       "      <td>86.42</td>\n",
       "      <td>3.12</td>\n",
       "      <td>[24002.010292360002, 37.06539515000023, 41.892...</td>\n",
       "      <td>137.19</td>\n",
       "      <td>2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>7</td>\n",
       "      <td>2.19</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.56</td>\n",
       "      <td>3</td>\n",
       "      <td>2.55</td>\n",
       "      <td>87.54</td>\n",
       "      <td>3.64</td>\n",
       "      <td>[27705.04026531, 46.45514194, 45.0791652500000...</td>\n",
       "      <td>265.27</td>\n",
       "      <td>6</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posterize_filter</th>\n",
       "      <td>9</td>\n",
       "      <td>1.93</td>\n",
       "      <td>91.04</td>\n",
       "      <td>3.01</td>\n",
       "      <td>[35.54594357000008, 35.64966141440052, 40.9539...</td>\n",
       "      <td>98.82</td>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>9</td>\n",
       "      <td>1.92</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.47</td>\n",
       "      <td>5</td>\n",
       "      <td>2.37</td>\n",
       "      <td>77.69</td>\n",
       "      <td>3.83</td>\n",
       "      <td>[42.10367875999998, 50.21577552035245, 5331.36...</td>\n",
       "      <td>136.22</td>\n",
       "      <td>1</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>static_snow_filter</th>\n",
       "      <td>10</td>\n",
       "      <td>2.03</td>\n",
       "      <td>100.00</td>\n",
       "      <td>3.13</td>\n",
       "      <td>[38.474057090000045, 36.687764187681516, 41.51...</td>\n",
       "      <td>93.45</td>\n",
       "      <td>0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>9</td>\n",
       "      <td>2.07</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51</td>\n",
       "      <td>7</td>\n",
       "      <td>2.00</td>\n",
       "      <td>89.33</td>\n",
       "      <td>3.26</td>\n",
       "      <td>[44.8215410900002, 42.57788347763431, 17936.84...</td>\n",
       "      <td>182.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Scale_0_Success  Scale_0_XTE Scale_0_Quickness  \\\n",
       "dynamic_snow_filter                  10         1.99            100.00   \n",
       "snow_filter                           8         1.59             83.06   \n",
       "histogram_equalisation                7         1.93             86.42   \n",
       "posterize_filter                      9         1.93             91.04   \n",
       "static_snow_filter                   10         2.03            100.00   \n",
       "\n",
       "                        Scale_0_MaxXTE  \\\n",
       "dynamic_snow_filter               2.96   \n",
       "snow_filter                       3.01   \n",
       "histogram_equalisation            3.12   \n",
       "posterize_filter                  3.01   \n",
       "static_snow_filter                3.13   \n",
       "\n",
       "                                                         Scale_0_Distance  \\\n",
       "dynamic_snow_filter     [40.51412019000011, 35.34271095772501, 18.2336...   \n",
       "snow_filter             [39.080562370000024, 38.156865552874464, 41.45...   \n",
       "histogram_equalisation  [24002.010292360002, 37.06539515000023, 41.892...   \n",
       "posterize_filter        [35.54594357000008, 35.64966141440052, 40.9539...   \n",
       "static_snow_filter      [38.474057090000045, 36.687764187681516, 41.51...   \n",
       "\n",
       "                        Scale_0_Time  Scale_0_Timeout  Scale_0_XTEderivative  \\\n",
       "dynamic_snow_filter            85.20                0                   0.07   \n",
       "snow_filter                    63.79                0                   0.08   \n",
       "histogram_equalisation        137.19                2                   0.20   \n",
       "posterize_filter               98.82                1                   0.21   \n",
       "static_snow_filter             93.45                0                   0.17   \n",
       "\n",
       "                        Scale_1_Success  Scale_1_XTE  ... Scale_3_Timeout  \\\n",
       "dynamic_snow_filter                  10         2.02  ...               2   \n",
       "snow_filter                           5         1.31  ...               0   \n",
       "histogram_equalisation                7         2.19  ...               5   \n",
       "posterize_filter                      9         1.92  ...               1   \n",
       "static_snow_filter                    9         2.07  ...               1   \n",
       "\n",
       "                        Scale_3_XTEderivative Scale_4_Success  Scale_4_XTE  \\\n",
       "dynamic_snow_filter                      0.18               6         2.36   \n",
       "snow_filter                              0.12               0         0.86   \n",
       "histogram_equalisation                   0.56               3         2.55   \n",
       "posterize_filter                         0.47               5         2.37   \n",
       "static_snow_filter                       0.51               7         2.00   \n",
       "\n",
       "                        Scale_4_Quickness  Scale_4_MaxXTE  \\\n",
       "dynamic_snow_filter                 79.85            3.81   \n",
       "snow_filter                         76.94            4.08   \n",
       "histogram_equalisation              87.54            3.64   \n",
       "posterize_filter                    77.69            3.83   \n",
       "static_snow_filter                  89.33            3.26   \n",
       "\n",
       "                                                         Scale_4_Distance  \\\n",
       "dynamic_snow_filter     [46.043610510000086, 21125.041126239987, 49.87...   \n",
       "snow_filter             [19264.174872740005, 7507.028343669046, 19641....   \n",
       "histogram_equalisation  [27705.04026531, 46.45514194, 45.0791652500000...   \n",
       "posterize_filter        [42.10367875999998, 50.21577552035245, 5331.36...   \n",
       "static_snow_filter      [44.8215410900002, 42.57788347763431, 17936.84...   \n",
       "\n",
       "                        Scale_4_Time Scale_4_Timeout  Scale_4_XTEderivative  \n",
       "dynamic_snow_filter           116.92               2                   0.56  \n",
       "snow_filter                     0.00               0                   0.13  \n",
       "histogram_equalisation        265.27               6                   0.57  \n",
       "posterize_filter              136.22               1                   0.43  \n",
       "static_snow_filter            182.00               3                   0.51  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def first_derivative(x):\n",
    "    return [x[i+1] - x[i] for i in range(len(x)-1)]\n",
    "\n",
    "# # Apply the first derivative to each XTE scale column\n",
    "df[\"Scale_0_XTEderivative\"] = df[\"Scale_0_XTE\"].apply(first_derivative)\n",
    "df[\"Scale_1_XTEderivative\"] = df[\"Scale_1_XTE\"].apply(first_derivative)\n",
    "df[\"Scale_2_XTEderivative\"] = df[\"Scale_2_XTE\"].apply(first_derivative)\n",
    "df[\"Scale_3_XTEderivative\"] = df[\"Scale_3_XTE\"].apply(first_derivative)\n",
    "df[\"Scale_4_XTEderivative\"] = df[\"Scale_4_XTE\"].apply(first_derivative)\n",
    "\n",
    "\n",
    "df[\"Scale_0_MaxXTE\"] = df[\"Scale_0_MaxXTE\"].apply(lambda x: (abs(i) for i in x))\n",
    "df[\"Scale_1_MaxXTE\"] = df[\"Scale_1_MaxXTE\"].apply(lambda x: (abs(i) for i in x))\n",
    "df[\"Scale_2_MaxXTE\"] = df[\"Scale_2_MaxXTE\"].apply(lambda x: (abs(i) for i in x))\n",
    "df[\"Scale_3_MaxXTE\"] = df[\"Scale_3_MaxXTE\"].apply(lambda x: (abs(i) for i in x))\n",
    "df[\"Scale_4_MaxXTE\"] = df[\"Scale_4_MaxXTE\"].apply(lambda x: (abs(i) for i in x))\n",
    "\n",
    "df[\"Scale_0_XTE\"] = df[\"Scale_0_XTE\"].apply(lambda x: (abs(i) for i in x))\n",
    "df[\"Scale_1_XTE\"] = df[\"Scale_1_XTE\"].apply(lambda x: (abs(i) for i in x))\n",
    "df[\"Scale_2_XTE\"] = df[\"Scale_2_XTE\"].apply(lambda x: (abs(i) for i in x))\n",
    "df[\"Scale_3_XTE\"] = df[\"Scale_3_XTE\"].apply(lambda x: (abs(i) for i in x))\n",
    "df[\"Scale_4_XTE\"] = df[\"Scale_4_XTE\"].apply(lambda x: (abs(i) for i in x))\n",
    "\n",
    "\n",
    "df[\"Scale_0_XTEderivative\"] = df[\"Scale_0_XTEderivative\"].apply(lambda x: (abs(i) for i in x))\n",
    "df[\"Scale_1_XTEderivative\"] = df[\"Scale_1_XTEderivative\"].apply(lambda x: (abs(i) for i in x))\n",
    "df[\"Scale_2_XTEderivative\"] = df[\"Scale_2_XTEderivative\"].apply(lambda x: (abs(i) for i in x))\n",
    "df[\"Scale_3_XTEderivative\"] = df[\"Scale_3_XTEderivative\"].apply(lambda x: (abs(i) for i in x))\n",
    "df[\"Scale_4_XTEderivative\"] = df[\"Scale_4_XTEderivative\"].apply(lambda x: (abs(i) for i in x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO\n",
    "df[\"Scale_0_Timeout\"] = df.apply(lambda row: [False if float(s)>0.93 else q for q, s in zip(row[\"Scale_0_Timeout\"], row[\"Scale_0_Quickness\"])], axis=1)\n",
    "df[\"Scale_1_Timeout\"] = df.apply(lambda row: [False if float(s)>0.93 else q for q, s in zip(row[\"Scale_1_Timeout\"], row[\"Scale_1_Quickness\"])], axis=1)\n",
    "df[\"Scale_2_Timeout\"] = df.apply(lambda row: [False if float(s)>0.93 else q for q, s in zip(row[\"Scale_2_Timeout\"], row[\"Scale_2_Quickness\"])], axis=1)\n",
    "df[\"Scale_3_Timeout\"] = df.apply(lambda row: [False if float(s)>0.93 else q for q, s in zip(row[\"Scale_3_Timeout\"], row[\"Scale_3_Quickness\"])], axis=1)\n",
    "df[\"Scale_4_Timeout\"] = df.apply(lambda row: [False if float(s)>0.93 else q for q, s in zip(row[\"Scale_4_Timeout\"], row[\"Scale_4_Quickness\"])], axis=1)\n",
    "# TODO\n",
    "\n",
    "\n",
    "df[\"Scale_0_Success\"] = df.apply(lambda row: [False if s else q for q, s in zip(row[\"Scale_0_Success\"], row[\"Scale_0_Timeout\"])], axis=1)\n",
    "df[\"Scale_1_Success\"] = df.apply(lambda row: [False if s else q for q, s in zip(row[\"Scale_1_Success\"], row[\"Scale_1_Timeout\"])], axis=1)\n",
    "df[\"Scale_2_Success\"] = df.apply(lambda row: [False if s else q for q, s in zip(row[\"Scale_2_Success\"], row[\"Scale_2_Timeout\"])], axis=1)\n",
    "df[\"Scale_3_Success\"] = df.apply(lambda row: [False if s else q for q, s in zip(row[\"Scale_3_Success\"], row[\"Scale_3_Timeout\"])], axis=1)\n",
    "df[\"Scale_4_Success\"] = df.apply(lambda row: [False if s else q for q, s in zip(row[\"Scale_4_Success\"], row[\"Scale_4_Timeout\"])], axis=1)\n",
    "\n",
    "\n",
    "# Adjust Quickness based on Success before calculating the mean\n",
    "df[\"Scale_0_Quickness\"] = df.apply(lambda row: [1 if s and not t else 1-q for q, s, t in zip(row[\"Scale_0_Quickness\"], row[\"Scale_0_Success\"], row[\"Scale_0_Timeout\"])], axis=1)\n",
    "df[\"Scale_1_Quickness\"] = df.apply(lambda row: [1 if s and not t else 1-q for q, s, t in zip(row[\"Scale_1_Quickness\"], row[\"Scale_1_Success\"], row[\"Scale_1_Timeout\"])], axis=1)\n",
    "df[\"Scale_2_Quickness\"] = df.apply(lambda row: [1 if s and not t else 1-q for q, s, t in zip(row[\"Scale_2_Quickness\"], row[\"Scale_2_Success\"], row[\"Scale_2_Timeout\"])], axis=1)\n",
    "df[\"Scale_3_Quickness\"] = df.apply(lambda row: [1 if s and not t else 1-q for q, s, t in zip(row[\"Scale_3_Quickness\"], row[\"Scale_3_Success\"], row[\"Scale_3_Timeout\"])], axis=1)\n",
    "df[\"Scale_4_Quickness\"] = df.apply(lambda row: [1 if s and not t else 1-q for q, s, t in zip(row[\"Scale_4_Quickness\"], row[\"Scale_4_Success\"], row[\"Scale_4_Timeout\"])], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# # Adjust Time based on Success and Timeout before calculating the mean\n",
    "df[\"Scale_0_Time\"] = df.apply(lambda row: [q if s or t else 0 for q, s, t in zip(row[\"Scale_0_Time\"], row[\"Scale_0_Success\"], row[\"Scale_0_Timeout\"])], axis=1)\n",
    "df[\"Scale_1_Time\"] = df.apply(lambda row: [q if s or t else 0 for q, s, t in zip(row[\"Scale_1_Time\"], row[\"Scale_1_Success\"], row[\"Scale_1_Timeout\"])], axis=1)\n",
    "df[\"Scale_2_Time\"] = df.apply(lambda row: [q if s or t else 0 for q, s, t in zip(row[\"Scale_2_Time\"], row[\"Scale_2_Success\"], row[\"Scale_2_Timeout\"])], axis=1)\n",
    "df[\"Scale_3_Time\"] = df.apply(lambda row: [q if s or t else 0 for q, s, t in zip(row[\"Scale_3_Time\"], row[\"Scale_3_Success\"], row[\"Scale_3_Timeout\"])], axis=1)\n",
    "df[\"Scale_4_Time\"] = df.apply(lambda row: [q if s or t else 0 for q, s, t in zip(row[\"Scale_4_Time\"], row[\"Scale_4_Success\"], row[\"Scale_4_Timeout\"])], axis=1)\n",
    "\n",
    "df[\"Scale_0_XTE\"] = df.apply(lambda row: [q if s or not t else 0. for q, s, t in zip(row[\"Scale_0_XTE\"], row[\"Scale_0_Success\"], row[\"Scale_0_Timeout\"])], axis=1)\n",
    "df[\"Scale_1_XTE\"] = df.apply(lambda row: [q if s or not t else 0. for q, s, t in zip(row[\"Scale_1_XTE\"], row[\"Scale_1_Success\"], row[\"Scale_1_Timeout\"])], axis=1)\n",
    "df[\"Scale_2_XTE\"] = df.apply(lambda row: [q if s or not t else 0. for q, s, t in zip(row[\"Scale_2_XTE\"], row[\"Scale_2_Success\"], row[\"Scale_2_Timeout\"])], axis=1)\n",
    "df[\"Scale_3_XTE\"] = df.apply(lambda row: [q if s or not t else 0. for q, s, t in zip(row[\"Scale_3_XTE\"], row[\"Scale_3_Success\"], row[\"Scale_3_Timeout\"])], axis=1)\n",
    "df[\"Scale_4_XTE\"] = df.apply(lambda row: [q if s or not t else 0. for q, s, t in zip(row[\"Scale_4_XTE\"], row[\"Scale_4_Success\"], row[\"Scale_4_Timeout\"])], axis=1)\n",
    "\n",
    "df[\"Scale_0_MaxXTE\"] = df.apply(lambda row: [q if s or not t else 0. for q, s, t in zip(row[\"Scale_0_MaxXTE\"], row[\"Scale_0_Success\"], row[\"Scale_0_Timeout\"])], axis=1)\n",
    "df[\"Scale_1_MaxXTE\"] = df.apply(lambda row: [q if s or not t else 0. for q, s, t in zip(row[\"Scale_1_MaxXTE\"], row[\"Scale_1_Success\"], row[\"Scale_1_Timeout\"])], axis=1)\n",
    "df[\"Scale_2_MaxXTE\"] = df.apply(lambda row: [q if s or not t else 0. for q, s, t in zip(row[\"Scale_2_MaxXTE\"], row[\"Scale_2_Success\"], row[\"Scale_2_Timeout\"])], axis=1)\n",
    "df[\"Scale_3_MaxXTE\"] = df.apply(lambda row: [q if s or not t else 0. for q, s, t in zip(row[\"Scale_3_MaxXTE\"], row[\"Scale_3_Success\"], row[\"Scale_3_Timeout\"])], axis=1)\n",
    "df[\"Scale_4_MaxXTE\"] = df.apply(lambda row: [q if s or not t else 0. for q, s, t in zip(row[\"Scale_4_MaxXTE\"], row[\"Scale_4_Success\"], row[\"Scale_4_Timeout\"])], axis=1)\n",
    "\n",
    "df[\"Scale_0_XTEderivative\"] = df.apply(lambda row: [float(q) if s or not t else 0. for q, s, t in zip(row[\"Scale_0_XTEderivative\"], row[\"Scale_0_Success\"], row[\"Scale_0_Timeout\"])], axis=1)\n",
    "df[\"Scale_1_XTEderivative\"] = df.apply(lambda row: [float(q) if s or not t else 0. for q, s, t in zip(row[\"Scale_1_XTEderivative\"], row[\"Scale_1_Success\"], row[\"Scale_1_Timeout\"])], axis=1)\n",
    "df[\"Scale_2_XTEderivative\"] = df.apply(lambda row: [float(q) if s or not t else 0. for q, s, t in zip(row[\"Scale_2_XTEderivative\"], row[\"Scale_2_Success\"], row[\"Scale_2_Timeout\"])], axis=1)\n",
    "df[\"Scale_3_XTEderivative\"] = df.apply(lambda row: [float(q) if s or not t else 0. for q, s, t in zip(row[\"Scale_3_XTEderivative\"], row[\"Scale_3_Success\"], row[\"Scale_3_Timeout\"])], axis=1)\n",
    "df[\"Scale_4_XTEderivative\"] = df.apply(lambda row: [float(q) if s or not t else 0. for q, s, t in zip(row[\"Scale_4_XTEderivative\"], row[\"Scale_4_Success\"], row[\"Scale_4_Timeout\"])], axis=1)\n",
    "\n",
    "\n",
    "df[\"Scale_0_Quickness\"] = df[\"Scale_0_Quickness\"].apply(lambda x: sum(x)*100 / len(x))\n",
    "df[\"Scale_0_Quickness\"] = df[\"Scale_0_Quickness\"].map(\"{:.2f}\".format)\n",
    "\n",
    "df[\"Scale_1_Quickness\"] = df[\"Scale_1_Quickness\"].apply(lambda x: sum(x)*100 / len(x))\n",
    "df[\"Scale_1_Quickness\"] = df[\"Scale_1_Quickness\"].map(\"{:.2f}\".format)\n",
    "\n",
    "df[\"Scale_2_Quickness\"] = df[\"Scale_2_Quickness\"].apply(lambda x: sum(x)*100 / len(x))\n",
    "df[\"Scale_2_Quickness\"] = df[\"Scale_2_Quickness\"].map(\"{:.2f}\".format)\n",
    "\n",
    "df[\"Scale_3_Quickness\"] = df[\"Scale_3_Quickness\"].apply(lambda x: sum(x)*100 / len(x))\n",
    "df[\"Scale_3_Quickness\"] = df[\"Scale_3_Quickness\"].map(\"{:.2f}\".format)\n",
    "\n",
    "df[\"Scale_4_Quickness\"] = df[\"Scale_4_Quickness\"].apply(lambda x: sum(x)*100 / len(x))\n",
    "df[\"Scale_4_Quickness\"] = df[\"Scale_4_Quickness\"].map(\"{:.2f}\".format)\n",
    "\n",
    "\n",
    "# For the Success columns, calculating the percentage of true values (as before)\n",
    "df[\"Scale_0_Success\"] = df[\"Scale_0_Success\"].apply(lambda x: sum(x))\n",
    "df[\"Scale_1_Success\"] = df[\"Scale_1_Success\"].apply(lambda x: sum(x))\n",
    "df[\"Scale_2_Success\"] = df[\"Scale_2_Success\"].apply(lambda x: sum(x))\n",
    "df[\"Scale_3_Success\"] = df[\"Scale_3_Success\"].apply(lambda x: sum(x))\n",
    "df[\"Scale_4_Success\"] = df[\"Scale_4_Success\"].apply(lambda x: sum(x))\n",
    "\n",
    "df[\"Scale_0_Timeout\"] = df[\"Scale_0_Timeout\"].apply(lambda x: sum(x))\n",
    "df[\"Scale_1_Timeout\"] = df[\"Scale_1_Timeout\"].apply(lambda x: sum(x))\n",
    "df[\"Scale_2_Timeout\"] = df[\"Scale_2_Timeout\"].apply(lambda x: sum(x))\n",
    "df[\"Scale_3_Timeout\"] = df[\"Scale_3_Timeout\"].apply(lambda x: sum(x))\n",
    "df[\"Scale_4_Timeout\"] = df[\"Scale_4_Timeout\"].apply(lambda x: sum(x))\n",
    "\n",
    "if type==\"donkey\":\n",
    "        df[\"Scale_0_XTEderivative\"] = df[\"Scale_0_XTEderivative\"].apply(lambda x: max(x)/2.5)\n",
    "        df[\"Scale_1_XTEderivative\"] = df[\"Scale_1_XTEderivative\"].apply(lambda x: max(x)/2.5)\n",
    "        df[\"Scale_2_XTEderivative\"] = df[\"Scale_2_XTEderivative\"].apply(lambda x: max(x)/2.5)\n",
    "        df[\"Scale_3_XTEderivative\"] = df[\"Scale_3_XTEderivative\"].apply(lambda x: max(x)/2.5)\n",
    "        df[\"Scale_4_XTEderivative\"] = df[\"Scale_4_XTEderivative\"].apply(lambda x: max(x)/2.5)\n",
    "else:\n",
    "        df[\"Scale_0_XTEderivative\"] = df[\"Scale_0_XTEderivative\"].apply(lambda x: max(x)/4.5)\n",
    "        df[\"Scale_1_XTEderivative\"] = df[\"Scale_1_XTEderivative\"].apply(lambda x: max(x)/4.5)\n",
    "        df[\"Scale_2_XTEderivative\"] = df[\"Scale_2_XTEderivative\"].apply(lambda x: max(x)/4.5)\n",
    "        df[\"Scale_3_XTEderivative\"] = df[\"Scale_3_XTEderivative\"].apply(lambda x: max(x)/4.5)\n",
    "        df[\"Scale_4_XTEderivative\"] = df[\"Scale_4_XTEderivative\"].apply(lambda x: max(x)/4.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"Scale_0_Time\"] = df[\"Scale_0_Time\"].apply(lambda x: sum([i for i in x if i != 0]) / len([i for i in x if i != 0]) if any(i != 0 for i in x) else 0)\n",
    "df[\"Scale_1_Time\"] = df[\"Scale_1_Time\"].apply(lambda x: sum([i for i in x if i != 0]) / len([i for i in x if i != 0]) if any(i != 0 for i in x) else 0)\n",
    "df[\"Scale_2_Time\"] = df[\"Scale_2_Time\"].apply(lambda x: sum([i for i in x if i != 0]) / len([i for i in x if i != 0]) if any(i != 0 for i in x) else 0)\n",
    "df[\"Scale_3_Time\"] = df[\"Scale_3_Time\"].apply(lambda x: sum([i for i in x if i != 0]) / len([i for i in x if i != 0]) if any(i != 0 for i in x) else 0)\n",
    "df[\"Scale_4_Time\"] = df[\"Scale_4_Time\"].apply(lambda x: sum([i for i in x if i != 0]) / len([i for i in x if i != 0]) if any(i != 0 for i in x) else 0)\n",
    "\n",
    "df[\"Scale_0_XTE\"] = df[\"Scale_0_XTE\"].apply(lambda x: sum([i for i in x if i != 0]) / len([i for i in x if i != 0]) if any(i != 0 for i in x) else 0)\n",
    "df[\"Scale_1_XTE\"] = df[\"Scale_1_XTE\"].apply(lambda x: sum([i for i in x if i != 0]) / len([i for i in x if i != 0]) if any(i != 0 for i in x) else 0)\n",
    "df[\"Scale_2_XTE\"] = df[\"Scale_2_XTE\"].apply(lambda x: sum([i for i in x if i != 0]) / len([i for i in x if i != 0]) if any(i != 0 for i in x) else 0)\n",
    "df[\"Scale_3_XTE\"] = df[\"Scale_3_XTE\"].apply(lambda x: sum([i for i in x if i != 0]) / len([i for i in x if i != 0]) if any(i != 0 for i in x) else 0)\n",
    "df[\"Scale_4_XTE\"] = df[\"Scale_4_XTE\"].apply(lambda x: sum([i for i in x if i != 0]) / len([i for i in x if i != 0]) if any(i != 0 for i in x) else 0)\n",
    "\n",
    "df[\"Scale_0_MaxXTE\"] = df[\"Scale_0_MaxXTE\"].apply(lambda x: sum([i for i in x if i != 0]) / len([i for i in x if i != 0]) if any(i != 0 for i in x) else 0)\n",
    "df[\"Scale_1_MaxXTE\"] = df[\"Scale_1_MaxXTE\"].apply(lambda x: sum([i for i in x if i != 0]) / len([i for i in x if i != 0]) if any(i != 0 for i in x) else 0)\n",
    "df[\"Scale_2_MaxXTE\"] = df[\"Scale_2_MaxXTE\"].apply(lambda x: sum([i for i in x if i != 0]) / len([i for i in x if i != 0]) if any(i != 0 for i in x) else 0)\n",
    "df[\"Scale_3_MaxXTE\"] = df[\"Scale_3_MaxXTE\"].apply(lambda x: sum([i for i in x if i != 0]) / len([i for i in x if i != 0]) if any(i != 0 for i in x) else 0)\n",
    "df[\"Scale_4_MaxXTE\"] = df[\"Scale_4_MaxXTE\"].apply(lambda x: sum([i for i in x if i != 0]) / len([i for i in x if i != 0]) if any(i != 0 for i in x) else 0)\n",
    "\n",
    "\n",
    "\n",
    "# Finally, rounding the entire dataframe to 2 decimal places\n",
    "df = df.round(2)\n",
    "\n",
    "# Displaying the updated dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is already created with columns such as 'Scale_0_XTE', 'Scale_0_Quickness', 'Scale_0_Success', etc.\n",
    "\n",
    "# Step 1: Create a backup of the original DataFrame\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Step 2: Extract the scale-related columns\n",
    "scale_0_columns = [col for col in df.columns if \"Scale_0\" in col and not \"RoadHash\" in col]\n",
    "scale_1_columns = [col for col in df.columns if \"Scale_1\" in col and not \"RoadHash\" in col]\n",
    "scale_2_columns = [col for col in df.columns if \"Scale_2\" in col and not \"RoadHash\" in col]\n",
    "scale_3_columns = [col for col in df.columns if \"Scale_3\" in col and not \"RoadHash\" in col]\n",
    "scale_4_columns = [col for col in df.columns if \"Scale_4\" in col and not \"RoadHash\" in col]\n",
    "\n",
    "# Step 3: Create the new reshaped DataFrame\n",
    "reshaped_df = pd.DataFrame()\n",
    "# Add Perturbation names (these are the index in the original DataFrame, so we reset it)\n",
    "reshaped_df[\"Perturbation\"] = pd.concat([df.index.to_series()] * 5, ignore_index=True)\n",
    "\n",
    "# Add Scale column\n",
    "reshaped_df[\"Scale\"] = [\"Scale_0\", \"Scale_1\", \"Scale_2\", \"Scale_3\", \"Scale_4\"] * len(df)\n",
    "\n",
    "# Extract XTE, Quickness, and Success into separate columns\n",
    "reshaped_df[\"XTE\"] = pd.concat([df[f\"Scale_{i}_XTE\"] for i in range(5)], ignore_index=True)\n",
    "reshaped_df[\"Quickness\"] = pd.concat([df[f\"Scale_{i}_Quickness\"] for i in range(5)], ignore_index=True)\n",
    "reshaped_df[\"Success\"] = pd.concat([df[f\"Scale_{i}_Success\"] for i in range(5)], ignore_index=True)\n",
    "reshaped_df[\"MaxXTE\"] = pd.concat([df[f\"Scale_{i}_MaxXTE\"] for i in range(5)], ignore_index=True)\n",
    "reshaped_df[\"XTEderivative\"] = pd.concat([df[f\"Scale_{i}_XTEderivative\"] for i in range(5)], ignore_index=True)\n",
    "reshaped_df[\"Time\"] = pd.concat([df[f\"Scale_{i}_Time\"] for i in range(5)], ignore_index=True)\n",
    "reshaped_df[\"Timeout\"] = pd.concat([df[f\"Scale_{i}_Timeout\"] for i in range(5)], ignore_index=True)\n",
    "\n",
    "# Format the columns to two decimal places and percentages for Success\n",
    "reshaped_df[\"XTE\"] = reshaped_df[\"XTE\"].astype(float).map(\"{:.2f}\".format)\n",
    "reshaped_df[\"Quickness\"] = reshaped_df[\"Quickness\"].astype(float).map(\"{:.2f}\".format)\n",
    "reshaped_df[\"Success\"] = reshaped_df[\"Success\"].astype(float).map(\"{:.2f}\".format)\n",
    "reshaped_df[\"Timeout\"] = reshaped_df[\"Timeout\"].astype(float).map(\"{:.2f}\".format)\n",
    "reshaped_df[\"MaxXTE\"] = reshaped_df[\"MaxXTE\"].astype(float).map(\"{:.2f}\".format)\n",
    "reshaped_df[\"XTEderivative\"] = reshaped_df[\"XTEderivative\"].astype(float).map(\"{:.2f}\".format)\n",
    "reshaped_df[\"Time\"] = reshaped_df[\"Time\"].astype(float).map(\"{:.2f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "names={\n",
    "    'gaussian_noise': '1 A-I',\n",
    "    'poisson_noise': '2 A-II',\n",
    "    'impulse_noise': '3 A-III',\n",
    "    'jpeg_filter': '4 A-IV',\n",
    "    'speckle_noise_filter': '5 A-V',\n",
    "    'defocus_blur': '6 B-I',\n",
    "    'motion_blur': '7 B-II',\n",
    "    # 'zoom_blur': 'DISCARD',\n",
    "    'gaussian_blur': '8 B-IV',\n",
    "    'low_pass_filter': '9 B-V',\n",
    "    'frost_filter': '10 C-I',\n",
    "    # 'glass_blur':'glass blur todo',\n",
    "    'snow_filter': '11 C-II',\n",
    "    'fog_filter': '12 C-III',\n",
    "    'increase_brightness': '13 C-IV',\n",
    "    'contrast': '14 C-V',\n",
    "    'elastic': '15 D-I',\n",
    "    'pixelate': '16 D-II',\n",
    "    'sample_pairing_filter': '17 D-III',\n",
    "    'sharpen_filter': '18 D-IV',\n",
    "    'scale_image': '19 E-II',\n",
    "    'translate_image': '20 E-III',\n",
    "    # 'reflection_filter': '21 reflection discard',\n",
    "    'splatter_mapping': '22 F-I',\n",
    "    'dotted_lines_mapping': '23 F-II',\n",
    "    'zigzag_mapping': '24 F-III',\n",
    "    'canny_edges_mapping': '25 F-IV',\n",
    "    'cutout_filter': '26 F-V',\n",
    "    'false_color_filter': '27 G-I',\n",
    "    'phase_scrambling': '28 G-II',\n",
    "    'histogram_equalisation': '29 G-III',\n",
    "    'white_balance_filter': '30 G-IV',\n",
    "    'grayscale_filter': '31 G-V',\n",
    "    'saturation_filter': '32 G-VI',\n",
    "    'saturation_decrease_filter': '33 G-VI',\n",
    "    'posterize_filter': '34 G-VII'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lccccccc}\n",
      "\\hline\n",
      "Perturbation & \\multicolumn{2}{c}{Completion Rate} & \\multicolumn{2}{c}{XTE} & \\multicolumn{2}{c}{Time} & Failure Type \\\\\n",
      " & Mean & Trend & Mean & Trend & Mean & Trend & \\\\\n",
      "\\hline\n",
      "1 A-I &            \\Chart{1.00}{1.00}{1.00}{0.94}{0.94} \\\\\n",
      "2 A-II &            \\Chart{1.00}{1.00}{1.00}{1.00}{1.00} \\\\\n",
      "3 A-III &            \\Chart{1.00}{1.00}{0.92}{0.94}{1.00} \\\\\n",
      "4 A-IV &            \\Chart{1.00}{0.92}{1.00}{0.77}{0.94} \\\\\n",
      "5 A-V &            \\Chart{0.99}{0.99}{0.99}{0.99}{0.93} \\\\\n",
      "6 B-I &            \\Chart{1.00}{0.95}{1.00}{1.00}{0.95} \\\\\n",
      "7 B-II &            \\Chart{1.00}{0.95}{1.00}{1.00}{1.00} \\\\\n",
      "8 B-IV &            \\Chart{1.00}{0.95}{0.95}{0.95}{1.00} \\\\\n",
      "9 B-V &            \\Chart{0.93}{0.99}{0.99}{0.99}{0.99} \\\\\n",
      "10 C-I &            \\Chart{1.00}{1.00}{0.76}{0.86}{0.74} \\\\\n",
      "11 C-II &            \\Chart{0.83}{0.70}{0.72}{0.70}{0.77} \\\\\n",
      "12 C-III &            \\Chart{1.00}{1.00}{0.79}{0.79}{0.78} \\\\\n",
      "13 C-IV &            \\Chart{1.00}{1.00}{0.95}{0.92}{0.92} \\\\\n",
      "14 C-V &            \\Chart{1.00}{1.00}{1.00}{0.88}{0.79} \\\\\n",
      "15 D-I &            \\Chart{1.00}{1.00}{1.00}{1.00}{0.92} \\\\\n",
      "16 D-II &            \\Chart{1.00}{1.00}{0.93}{1.00}{1.00} \\\\\n",
      "17 D-III &            \\Chart{1.00}{1.00}{0.90}{0.79}{0.67} \\\\\n",
      "18 D-IV &            \\Chart{1.00}{0.94}{1.00}{0.66}{0.70} \\\\\n",
      "19 E-II &            \\Chart{1.00}{1.00}{0.88}{0.94}{0.73} \\\\\n",
      "20 E-III &            \\Chart{0.95}{0.72}{0.70}{0.78}{0.84} \\\\\n",
      "22 F-I &            \\Chart{0.94}{0.95}{0.94}{0.78}{0.58} \\\\\n",
      "23 F-II &            \\Chart{1.00}{1.00}{0.92}{0.95}{0.86} \\\\\n",
      "24 F-III &            \\Chart{1.00}{1.00}{1.00}{1.00}{1.00} \\\\\n",
      "25 F-IV &            \\Chart{0.93}{0.99}{0.99}{0.99}{0.99} \\\\\n",
      "26 F-V &            \\Chart{1.00}{0.94}{1.00}{0.85}{0.79} \\\\\n",
      "27 G-I &            \\Chart{0.71}{0.99}{0.78}{0.92}{0.81} \\\\\n",
      "28 G-II &            \\Chart{0.99}{0.99}{0.93}{0.54}{0.74} \\\\\n",
      "29 G-III &            \\Chart{0.86}{0.99}{0.90}{0.91}{0.88} \\\\\n",
      "30 G-IV &            \\Chart{1.00}{1.00}{1.00}{1.00}{0.92} \\\\\n",
      "31 G-V &            \\Chart{1.00}{0.95}{1.00}{0.78}{0.70} \\\\\n",
      "32 G-VI &            \\Chart{0.94}{0.94}{0.89}{0.71}{0.68} \\\\\n",
      "33 G-VI &            \\Chart{1.00}{1.00}{1.00}{0.68}{0.78} \\\\\n",
      "34 G-VII &            \\Chart{0.91}{1.00}{1.00}{1.00}{0.78} \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = reshaped_df.copy()\n",
    "\n",
    "# Step 1: Convert XTE, Quickness, Time, Success, and Timeout to numeric\n",
    "df['XTE'] = pd.to_numeric(df['XTE'], errors='coerce')\n",
    "df['MaxXTE'] = pd.to_numeric(df['MaxXTE'], errors='coerce')  # Convert Timeout to numeric\n",
    "df['XTEderivative'] = pd.to_numeric(df['XTEderivative'], errors='coerce')  # Convert Timeout to numeric\n",
    "df['Quickness'] = pd.to_numeric(df['Quickness'], errors='coerce')  # Now used as completion rate\n",
    "df['Time'] = pd.to_numeric(df['Time'], errors='coerce')  # Replacing Success with Time\n",
    "df['Success'] = pd.to_numeric(df['Success'], errors='coerce')  # Convert Success to numeric\n",
    "df['Timeout'] = pd.to_numeric(df['Timeout'], errors='coerce')  # Convert Timeout to numeric\n",
    "\n",
    "# Step 2: Function to calculate failure type (T and O based on sum of Success and Timeout)\n",
    "def calculate_failure_type(df):\n",
    "    # Group by Perturbation and sum Success and Timeout\n",
    "    grouped = df.groupby('Perturbation').agg(\n",
    "        Success_Sum=('Success', 'sum'),\n",
    "        Timeout_Sum=('Timeout', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate O and T values\n",
    "    grouped['O'] = 50-(grouped['Success_Sum'])-grouped['Timeout_Sum']\n",
    "    grouped['T'] = grouped['Timeout_Sum']\n",
    "\n",
    "    # Convert to string format: \"O:O_Value/T:T_Value\"\n",
    "    grouped['failure_type'] = \"O:\" + grouped['O'].astype(int).astype(str) + \"/T:\" + grouped['T'].astype(int).astype(str)\n",
    "    \n",
    "    return grouped[['Perturbation', 'failure_type']]\n",
    "\n",
    "# Add the new 'failure_type' column\n",
    "failure_type_df = calculate_failure_type(df)\n",
    "\n",
    "# Merge the failure type back into the summary DataFrame\n",
    "df = df.merge(failure_type_df, on='Perturbation')\n",
    "\n",
    "# Now that the failure type is added, proceed with the previous steps\n",
    "\n",
    "# # Function to normalize values for specific metrics\n",
    "def normalize_values(values, metric):\n",
    "    if metric == 'XTE':\n",
    "        # Normalize XTE where 0 maps to 0 and 4 maps to 1\n",
    "        return [(min((val / 4.1),4)) for val in values]\n",
    "    \n",
    "    elif metric == 'Success':  # Now used as completion rate\n",
    "        # Normalize Quickness where 0 maps to 0 and 1 maps to 1\n",
    "        return values/10  # Since it's already between 0 and 1\n",
    "\n",
    "    elif metric == 'Quickness':  # Now used as completion rate\n",
    "        # Normalize Quickness where 0 maps to 0 and 1 maps to 1\n",
    "        return values/100  # Since it's already between 0 and 1\n",
    "    \n",
    "    elif metric == 'Time':\n",
    "        # Normalize Time, treating it as is (higher times are worse)\n",
    "        max_time = 150\n",
    "        return [val / max_time if val<150 else 1 for val in values]\n",
    "    \n",
    "    else:\n",
    "        return values\n",
    "\n",
    "# def normalize_values(values,dummy):\n",
    "#     min_value = np.min(values)\n",
    "#     max_value = np.max(values)\n",
    "    \n",
    "#     if min_value == max_value:\n",
    "#         return [0.5 for _ in values]\n",
    "    \n",
    "#     return [(val - min_value) / (max_value - min_value) for val in values]\n",
    "\n",
    "# Function to escape special LaTeX characters in Perturbation names\n",
    "def escape_latex_special_chars(text):\n",
    "    special_chars = {\n",
    "        '&': r'\\&',\n",
    "        '%': r'\\%',\n",
    "        '$': r'\\$',\n",
    "        '#': r'\\#',\n",
    "        '_': r'\\_',\n",
    "        '{': r'\\{',\n",
    "        '}': r'\\}',\n",
    "        '~': r'\\textasciitilde',\n",
    "        '^': r'\\textasciicircum',\n",
    "        '\\\\': r'\\textbackslash'\n",
    "    }\n",
    "    return ''.join(special_chars.get(c, c) for c in text)\n",
    "\n",
    "# Step 3: Function to generate LaTeX \\Chart{} command for each perturbation and metric\n",
    "def generate_chart_latex(perturbation, metric, df):\n",
    "    \"\"\"Generate the LaTeX \\Chart{} command for the given perturbation and metric across all scales.\"\"\"\n",
    "    scale_values = df[df['Perturbation'] == perturbation][metric].values\n",
    "    \n",
    "    # Normalize the values for the LaTeX barplot\n",
    "    normalized_values = normalize_values(scale_values, metric)\n",
    "    \n",
    "    # Convert normalized values to LaTeX \\Chart command with 5 bars\n",
    "    chart_latex = f\"\\\\Chart{{{normalized_values[0]:.2f}}}{{{normalized_values[1]:.2f}}}{{{normalized_values[2]:.2f}}}{{{normalized_values[3]:.2f}}}{{{normalized_values[4]:.2f}}}\"\n",
    "    \n",
    "    return chart_latex\n",
    "\n",
    "# Step 4: Group by Perturbation and calculate mean\n",
    "df_summary = df.groupby('Perturbation').agg(\n",
    "    XTE_Mean=('XTE', 'mean'),\n",
    "    Quickness_Mean=('Quickness', 'mean'),  # This will be used as the completion rate\n",
    "    Time_Mean=('Time', 'mean'),  # Use the Time column for average time\n",
    "    Failure_Type=('failure_type', 'first')  # Get the failure_type\n",
    ").reset_index()\n",
    "\n",
    "# Step 5: Generate \\Chart{} for each perturbation and metric\n",
    "df_summary['Completion_Rate_Chart'] = df_summary['Perturbation'].apply(lambda p: generate_chart_latex(p, 'Quickness', df))  # Completion rate\n",
    "df_summary['XTE_Chart'] = df_summary['Perturbation'].apply(lambda p: generate_chart_latex(p, 'XTE', df))\n",
    "df_summary['Time_Chart'] = df_summary['Perturbation'].apply(lambda p: generate_chart_latex(p, 'Time', df))  # Time instead of Success\n",
    "\n",
    "# Step 6: Create the LaTeX table with means, inline \\Chart{}, and failure type\n",
    "def dataframe_to_latex_with_charts(df):\n",
    "    latex_lines = []\n",
    "    latex_lines.append(\"\\\\begin{tabular}{lccccccc}\")\n",
    "    latex_lines.append(\"\\\\hline\")\n",
    "    latex_lines.append(\"Perturbation & \\\\multicolumn{2}{c}{Completion Rate} & \\\\multicolumn{2}{c}{XTE} & \\\\multicolumn{2}{c}{Time} & Failure Type \\\\\\\\\")\n",
    "    latex_lines.append(\" & Mean & Trend & Mean & Trend & Mean & Trend & \\\\\\\\\")\n",
    "    latex_lines.append(\"\\\\hline\")\n",
    "\n",
    "    # Add rows for each perturbation\n",
    "    for idx, row in df.iterrows():\n",
    "        perturbation = row['Perturbation']\n",
    "        \n",
    "        # # Add row to the LaTeX table with the updated order of Completion Rate, XTE, Time, and Failure Type\n",
    "        # latex_lines.append(f\"{names[perturbation]} & {(row['Quickness_Mean']):.2f} & {row['Completion_Rate_Chart']} & \"\n",
    "        #                    f\"{min(4.0,row['XTE_Mean']):.2f} & {row['XTE_Chart']} & \"\n",
    "        #                    f\"{row['Time_Mean']:.2f} & {row['Time_Chart']} & \"\n",
    "        #                    f\"{row['Failure_Type']} \\\\\\\\\")\n",
    "        \n",
    "        # Add row to the LaTeX table with the updated order of Completion Rate, XTE, Time, and Failure Type\n",
    "        latex_lines.append(f\"{names[perturbation]} &            {row['Completion_Rate_Chart']} \\\\\\\\\")\n",
    "    \n",
    "    latex_lines.append(\"\\\\hline\")\n",
    "    latex_lines.append(\"\\\\end{tabular}\")\n",
    "\n",
    "    return \"\\n\".join(latex_lines)\n",
    "\n",
    "\n",
    "order_array = list(names.keys())\n",
    "# print(order_array)\n",
    "\n",
    "# Set the custom order using pd.Categorical\n",
    "df_summary['Perturbation'] = pd.Categorical(df_summary['Perturbation'], categories=order_array, ordered=True)\n",
    "\n",
    "# Sort the DataFrame by the 'Perturbation' column using the custom order\n",
    "df_sorted = df_summary.sort_values('Perturbation')\n",
    "\n",
    "df_sorted.dropna(subset=['Perturbation'], inplace=True)\n",
    "\n",
    "# Generate LaTeX table with inline barplots and failure type\n",
    "latex_table = dataframe_to_latex_with_charts(df_sorted)\n",
    "\n",
    "# Output the LaTeX table (you can write it to a file or print)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lcccccccccccc}\n",
      "\\hline\n",
      "Perturbation & \\multicolumn{2}{c}{Completion Rate} & \\multicolumn{2}{c}{Failure Type} & \\multicolumn{2}{c}{Driving Quality (XTE)} & \\multicolumn{2}{c}{Time of Execution} \\\\\n",
      " & Mean & Variance & O & T & Mean & Variance & Mean & Variance \\\\\n",
      "\\hline\n",
      "         & 86\\% & 3.00 & \\Chart{0.90}{0.90}{0.90}{0.80}{0.80} & 2 & 5 & 95.6 & 39.2 & 44.8\\% & 0.16 \\\\ \n",
      "         & 100\\% & 0.00 & \\Chart{1.00}{1.00}{1.00}{1.00}{1.00} & 0 & 0 & 50.2 & 3.2 & 9.4\\% & 0.35 \\\\ \n",
      "         & 96\\% & 3.00 & \\Chart{1.00}{1.00}{0.90}{0.90}{1.00} & 0 & 2 & 129.9 & 51.9 & 11.0\\% & 0.20 \\\\ \n",
      "         & 66\\% & 88.00 & \\Chart{1.00}{0.80}{0.80}{0.40}{0.30} & 1 & 16 & 149.2 & 3007.4 & 38.8\\% & 3.06 \\\\ \n",
      "         & 88\\% & 2.00 & \\Chart{0.90}{0.90}{0.90}{0.90}{0.80} & 6 & 0 & 59.2 & 0.7 & 20.0\\% & 0.04 \\\\ \n",
      "         & 96\\% & 3.00 & \\Chart{1.00}{0.90}{1.00}{1.00}{0.90} & 1 & 1 & 103.4 & 303.9 & 12.6\\% & 0.15 \\\\ \n",
      "         & 84\\% & 8.00 & \\Chart{1.00}{0.80}{0.80}{0.80}{0.80} & 0 & 8 & 122.2 & 865.9 & 46.0\\% & 3.06 \\\\ \n",
      "         & 94\\% & 3.00 & \\Chart{1.00}{0.90}{0.90}{0.90}{1.00} & 3 & 0 & 87.6 & 100.6 & 11.8\\% & 0.25 \\\\ \n",
      "         & 88\\% & 2.00 & \\Chart{0.80}{0.90}{0.90}{0.90}{0.90} & 6 & 0 & 88.0 & 12.0 & 28.6\\% & 0.00 \\\\ \n",
      "         & 70\\% & 135.00 & \\Chart{1.00}{1.00}{0.70}{0.70}{0.10} & 15 & 0 & 52.1 & 94.3 & 11.6\\% & 0.03 \\\\ \n",
      "         & 30\\% & 120.00 & \\Chart{0.80}{0.50}{0.20}{0.00}{0.00} & 35 & 0 & 32.5 & 924.2 & 13.8\\% & 0.20 \\\\ \n",
      "         & 54\\% & 228.00 & \\Chart{1.00}{1.00}{0.60}{0.10}{0.00} & 23 & 0 & 52.2 & 1012.8 & 10.6\\% & 0.14 \\\\ \n",
      "         & 92\\% & 7.00 & \\Chart{1.00}{1.00}{0.90}{0.90}{0.80} & 3 & 1 & 123.2 & 235.4 & 12.2\\% & 0.17 \\\\ \n",
      "         & 90\\% & 20.00 & \\Chart{1.00}{1.00}{1.00}{0.80}{0.70} & 4 & 1 & 92.5 & 322.6 & 16.4\\% & 0.21 \\\\ \n",
      "         & 98\\% & 2.00 & \\Chart{1.00}{1.00}{1.00}{1.00}{0.90} & 0 & 1 & 73.3 & 154.3 & 9.6\\% & 0.27 \\\\ \n",
      "         & 96\\% & 8.00 & \\Chart{1.00}{1.00}{0.80}{1.00}{1.00} & 2 & 0 & 84.4 & 7.2 & 14.4\\% & 0.35 \\\\ \n",
      "         & 54\\% & 213.00 & \\Chart{1.00}{0.90}{0.70}{0.10}{0.00} & 21 & 2 & 86.6 & 2543.7 & 29.6\\% & 3.92 \\\\ \n",
      "         & 60\\% & 255.00 & \\Chart{1.00}{0.90}{1.00}{0.10}{0.00} & 20 & 0 & 64.0 & 1496.5 & 18.6\\% & 0.74 \\\\ \n",
      "         & 70\\% & 45.00 & \\Chart{0.90}{0.90}{0.60}{0.70}{0.40} & 5 & 10 & 220.7 & 3958.7 & 24.6\\% & 0.71 \\\\ \n",
      "         & 20\\% & 120.00 & \\Chart{0.80}{0.00}{0.20}{0.00}{0.00} & 29 & 11 & 323.8 & 14913.8 & 22.2\\% & 0.41 \\\\ \n",
      "         & 58\\% & 122.00 & \\Chart{0.80}{0.80}{0.80}{0.50}{0.00} & 16 & 5 & 156.8 & 8826.7 & 16.0\\% & 0.14 \\\\ \n",
      "         & 92\\% & 7.00 & \\Chart{1.00}{1.00}{0.90}{0.90}{0.80} & 3 & 1 & 92.6 & 237.6 & 18.6\\% & 0.56 \\\\ \n",
      "         & 100\\% & 0.00 & \\Chart{1.00}{1.00}{1.00}{1.00}{1.00} & 0 & 0 & 77.2 & 9.7 & 17.4\\% & 0.13 \\\\ \n",
      "         & 88\\% & 2.00 & \\Chart{0.80}{0.90}{0.90}{0.90}{0.90} & 6 & 0 & 90.0 & 2.5 & 19.4\\% & 0.01 \\\\ \n",
      "         & 84\\% & 43.00 & \\Chart{1.00}{0.90}{1.00}{0.80}{0.50} & 8 & 0 & 123.4 & 731.7 & 12.2\\% & 0.38 \\\\ \n",
      "         & 24\\% & 153.00 & \\Chart{0.00}{0.90}{0.00}{0.00}{0.30} & 38 & 0 & 40.8 & 3230.7 & 14.4\\% & 0.57 \\\\ \n",
      "         & 56\\% & 113.00 & \\Chart{0.90}{0.80}{0.70}{0.20}{0.20} & 19 & 3 & 44.3 & 78.9 & 33.4\\% & 2.10 \\\\ \n",
      "         & 52\\% & 32.00 & \\Chart{0.70}{0.70}{0.50}{0.40}{0.30} & 6 & 18 & 197.5 & 2127.2 & 49.0\\% & 2.67 \\\\ \n",
      "         & 94\\% & 8.00 & \\Chart{1.00}{1.00}{1.00}{0.90}{0.80} & 0 & 3 & 91.9 & 559.3 & 29.6\\% & 3.32 \\\\ \n",
      "         & 66\\% & 173.00 & \\Chart{1.00}{0.90}{0.90}{0.50}{0.00} & 13 & 4 & 143.8 & 3136.5 & 32.0\\% & 1.59 \\\\ \n",
      "         & 68\\% & 72.00 & \\Chart{0.90}{0.90}{0.80}{0.50}{0.30} & 11 & 5 & 118.8 & 2413.4 & 22.4\\% & 1.09 \\\\ \n",
      "         & 52\\% & 187.00 & \\Chart{0.90}{0.80}{0.80}{0.10}{0.00} & 15 & 9 & 103.6 & 123.9 & 17.6\\% & 1.79 \\\\ \n",
      "         & 82\\% & 32.00 & \\Chart{0.90}{0.90}{0.90}{0.90}{0.50} & 4 & 5 & 108.1 & 259.0 & 40.2\\% & 1.17 \\\\ \n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Group by Perturbation and calculate mean and variance\n",
    "df_summary = df.groupby('Perturbation').agg(\n",
    "    XTE_Mean=('XTEderivative', 'mean'),\n",
    "    XTE_Var=('XTEderivative', 'var'),\n",
    "    Quickness_Mean=('Success', 'mean'),  # Completion rate mean\n",
    "    Quickness_Var=('Success', 'var'),  # Completion rate variance\n",
    "    Time_Mean=('Time', 'mean'),  # Time mean\n",
    "    Time_Var=('Time', 'var'),  # Time variance\n",
    "    Success_Sum=('Success', 'sum'),\n",
    "    Timeout_Sum=('Timeout', 'sum')  # Use the sum of timeout and success for failure type\n",
    ").reset_index()\n",
    "\n",
    "# Calculate O and T values for failure type\n",
    "df_summary['O'] = 50 - df_summary['Success_Sum']-df_summary['Timeout_Sum']\n",
    "df_summary['T'] = df_summary['Timeout_Sum']\n",
    "\n",
    "df_summary['Completion_Rate_Chart'] = df_summary['Perturbation'].apply(lambda p: generate_chart_latex(p, 'Success', df))  # Completion rate\n",
    "\n",
    "# Step 5: Generate \\Chart{} for each perturbation and metric (if needed, here omitted)\n",
    "# You can add this if you need the charts for trends in columns like completion rate or driving quality\n",
    "\n",
    "# Step 6: Create the LaTeX table with mean, variance, and failure type\n",
    "def dataframe_to_latex_with_charts_v2(df):\n",
    "    latex_lines = []\n",
    "    latex_lines.append(\"\\\\begin{tabular}{lcccccccccccc}\")\n",
    "    latex_lines.append(\"\\\\hline\")\n",
    "    latex_lines.append(\"Perturbation & \\\\multicolumn{2}{c}{Completion Rate} & \\\\multicolumn{2}{c}{Failure Type} & \\\\multicolumn{2}{c}{Driving Quality (XTE)} & \\\\multicolumn{2}{c}{Time of Execution} \\\\\\\\\")\n",
    "    latex_lines.append(\" & Mean & Variance & O & T & Mean & Variance & Mean & Variance \\\\\\\\\")\n",
    "    latex_lines.append(\"\\\\hline\")\n",
    "\n",
    "    # Add rows for each perturbation\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['Perturbation'] not in ['candy','dynamic_lightning_filter','dynamic_object_overlay','dynamic_rain_filter','dynamic_smoke_filter','dynamic_snow_filter','dynamic_sun_filter','effects_attention_rain','effects_attention_rain_dynamic','effects_rain_dynamic','effects_snowflake_dynamic','feathers','la_muse','mosaic','static_lightning_filter','static_object_overlay','static_rain_filter','static_smoke_filter','static_snow_filter','static_sun_filter','the_scream','udnie','reflection_filter']:\n",
    "            perturbation =names[row['Perturbation']]\n",
    "            if type==\"udacity\":\n",
    "                # Add row to the LaTeX table\n",
    "                latex_lines.append(f\"         & \"\n",
    "                                f\"{(row['Quickness_Mean'])*10:.0f}\\% & {row['Quickness_Var']*10:.2f} & {row['Completion_Rate_Chart']} & \"  # Completion Rate: Mean and Variance\n",
    "                                f\"{int(row['O'])} & {int(row['T'])} & \"  # Failure Type: O and T\n",
    "                                f\"{row['Time_Mean']:.1f} & {row['Time_Var']:.1f} & \"\n",
    "                                f\"{min(1.0, (row['XTE_Mean']))*100:.1f}\\% & {row['XTE_Var']*100:.2f} \\\\\\\\ \") \n",
    "                                # Time of Execution: Mean and Variance\n",
    "            else:\n",
    "                # Add row to the LaTeX table\n",
    "                latex_lines.append(f\"         & \"\n",
    "                                f\"{(row['Quickness_Mean'])*10:.0f}\\% & {row['Quickness_Var']*10:.2f} & {row['Completion_Rate_Chart']} &\"  # Completion Rate: Mean and Variance\n",
    "                                f\"{int(row['O'])} & {int(row['T'])} & \"  # Failure Type: O and T\n",
    "                                f\"{row['Time_Mean']:.1f} & {row['Time_Var']:.1f} & \"\n",
    "                                f\"{min(1.0, (row['XTE_Mean']))*100:.1f}\\% & {row['XTE_Var']*100:.2f} \") \n",
    "                                # Time of Execution: Mean and Variance\n",
    "    \n",
    "    latex_lines.append(\"\\\\hline\")\n",
    "    latex_lines.append(\"\\\\end{tabular}\")\n",
    "\n",
    "    return \"\\n\".join(latex_lines)\n",
    "\n",
    "\n",
    "order_array = list(names.keys())\n",
    "# print(order_array)\n",
    "\n",
    "# Set the custom order using pd.Categorical\n",
    "df_summary['Perturbation'] = pd.Categorical(df_summary['Perturbation'], categories=order_array, ordered=True)\n",
    "\n",
    "# Sort the DataFrame by the 'Perturbation' column using the custom order\n",
    "df_sorted = df_summary.sort_values('Perturbation')\n",
    "\n",
    "df_sorted.dropna(subset=['Perturbation'], inplace=True)\n",
    "# print(df_sorted)\n",
    "\n",
    "# Generate LaTeX table with the new structure\n",
    "latex_table_v2 = dataframe_to_latex_with_charts_v2(df_sorted)\n",
    "\n",
    "# Output the LaTeX table\n",
    "print(latex_table_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A-I & 0.43 & 0.16 & 0.64 & 0.23                       & 98\\% & 2.00 & \\Chart{1.00}{1.00}{1.00}{1.00}{0.90} &0 & 1 & 83.3 & 229.9 & 9.4\\% & 0.25                  & 86\\% & 3.00 & \\Chart{0.90}{0.90}{0.90}{0.80}{0.80} & 2 & 5 & 95.6 & 39.2 & 44.8\\% & 0.16 \\\\ \n",
    "# A-II & 0.50 & 0.10 & 0.58 & 0.32                      & 98\\% & 2.00 & \\Chart{1.00}{1.00}{1.00}{1.00}{0.90} &1 & 0 & 52.2 & 3.5 & 9.0\\% & 0.20                    & 100\\% & 0.00 & \\Chart{1.00}{1.00}{1.00}{1.00}{1.00} & 0 & 0 & 50.2 & 3.2 & 9.4\\% & 0.35 \\\\ \n",
    "# A-III & 0.41 & 0.16 & 0.62 & 0.21                     & 100\\% & 0.00 & \\Chart{1.00}{1.00}{1.00}{1.00}{1.00} &0 & 0 & 95.5 & 130.1 & 7.6\\% & 0.03                 & 96\\% & 3.00 & \\Chart{1.00}{1.00}{0.90}{0.90}{1.00} & 0 & 2 & 129.9 & 51.9 & 11.0\\% & 0.20 \\\\ \n",
    "# A-IV & 0.59 & 0.02 & 0.62 & 0.56                      & 98\\% & 2.00 & \\Chart{0.90}{1.00}{1.00}{1.00}{1.00} &1 & 0 & 77.3 & 879.4 & 4.8\\% & 0.06                  & 66\\% & 88.00 & \\Chart{1.00}{0.80}{0.80}{0.40}{0.30} & 1 & 16 & 149.2 & 3007.4 & 38.8\\% & 3.06 \\\\ \n",
    "# A-V & 0.58 & 0.09 & 0.66 & 0.41                       & 100\\% & 0.00 & \\Chart{1.00}{1.00}{1.00}{1.00}{1.00} &0 & 0 & 73.4 & 5.3 & 6.2\\% & 0.01                   & 88\\% & 2.00 & \\Chart{0.90}{0.90}{0.90}{0.90}{0.80} & 6 & 0 & 59.2 & 0.7 & 20.0\\% & 0.04 \\\\ \n",
    "# B-I & 0.57 & 0.06 & 0.65 & 0.47                       & 100\\% & 0.00 & \\Chart{1.00}{1.00}{1.00}{1.00}{1.00} &0 & 0 & 84.6 & 18.1 & 8.0\\% & 0.00                  & 96\\% & 3.00 & \\Chart{1.00}{0.90}{1.00}{1.00}{0.90} & 1 & 1 & 103.4 & 303.9 & 12.6\\% & 0.15 \\\\ \n",
    "# B-II & 0.63 & 0.03 & 0.66 & 0.58                      & 94\\% & 8.00 & \\Chart{1.00}{0.80}{0.90}{1.00}{1.00} &3 & 0 & 93.8 & 39.2 & 8.6\\% & 0.11                   & 84\\% & 8.00 & \\Chart{1.00}{0.80}{0.80}{0.80}{0.80} & 0 & 8 & 122.2 & 865.9 & 46.0\\% & 3.06 \\\\ \n",
    "# B-IV & 0.58 & 0.08 & 0.66 & 0.46                      & 94\\% & 3.00 & \\Chart{1.00}{0.90}{0.90}{0.90}{1.00} &3 & 0 & 99.9 & 57.6 & 9.0\\% & 0.18                   & 94\\% & 3.00 & \\Chart{1.00}{0.90}{0.90}{0.90}{1.00} & 3 & 0 & 87.6 & 100.6 & 11.8\\% & 0.25 \\\\ \n",
    "# B-V & 0.63 & 0.01 & 0.64 & 0.62                       & 96\\% & 3.00 & \\Chart{1.00}{1.00}{1.00}{0.90}{0.90} &2 & 0 & 92.7 & 1.9 & 8.6\\% & 0.07                    & 88\\% & 2.00 & \\Chart{0.80}{0.90}{0.90}{0.90}{0.90} & 6 & 0 & 88.0 & 12.0 & 28.6\\% & 0.00 \\\\ \n",
    "# C-I & 0.62 & 0.04 & 0.65 & 0.54                       & 94\\% & 3.00 & \\Chart{0.90}{0.90}{0.90}{1.00}{1.00} &3 & 0 & 69.3 & 5.2 & 7.8\\% & 0.03                    & 70\\% & 135.00 & \\Chart{1.00}{1.00}{0.70}{0.70}{0.10} & 15 & 0 & 52.1 & 94.3 & 11.6\\% & 0.03 \\\\ \n",
    "# C-II & 0.64 & 0.04 & 0.67 & 0.56                      & 96\\% & 8.00 & \\Chart{1.00}{1.00}{1.00}{1.00}{0.80} &2 & 0 & 74.9 & 18.7 & 10.2\\% & 0.15                  & 30\\% & 120.00 & \\Chart{0.80}{0.50}{0.20}{0.00}{0.00} & 35 & 0 & 32.5 & 924.2 & 13.8\\% & 0.20 \\\\ \n",
    "# C-III & 0.56 & 0.14 & 0.67 & 0.29                     & 90\\% & 5.00 & \\Chart{0.90}{0.90}{1.00}{0.90}{0.80} &5 & 0 & 81.7 & 69.6 & 11.6\\% & 0.35                  & 54\\% & 228.00 & \\Chart{1.00}{1.00}{0.60}{0.10}{0.00} & 23 & 0 & 52.2 & 1012.8 & 10.6\\% & 0.14 \\\\ \n",
    "# C-IV & 0.66 & 0.00 & 0.66 & 0.65                      & 96\\% & 8.00 & \\Chart{1.00}{1.00}{1.00}{0.80}{1.00} &2 & 0 & 95.0 & 94.3 & 10.4\\% & 0.15                  & 92\\% & 7.00 & \\Chart{1.00}{1.00}{0.90}{0.90}{0.80} & 3 & 1 & 123.2 & 235.4 & 12.2\\% & 0.17 \\\\ \n",
    "# C-V & 0.66 & 0.00 & 0.66 & 0.65                       & 52\\% & 197.00 & \\Chart{1.00}{0.80}{0.70}{0.10}{0.00} &17 & 7 & 91.8 & 2971.1 & 13.8\\% & 1.19             & 90\\% & 20.00 & \\Chart{1.00}{1.00}{1.00}{0.80}{0.70} & 4 & 1 & 92.5 & 322.6 & 16.4\\% & 0.21 \\\\ \n",
    "# D-I & 0.63 & 0.01 & 0.64 & 0.61                       & 100\\% & 0.00 & \\Chart{1.00}{1.00}{1.00}{1.00}{1.00} &0 & 0 & 85.1 & 5.0 & 6.6\\% & 0.16                   & 98\\% & 2.00 & \\Chart{1.00}{1.00}{1.00}{1.00}{0.90} & 0 & 1 & 73.3 & 154.3 & 9.6\\% & 0.27 \\\\ \n",
    "# D-II & 0.62 & 0.05 & 0.66 & 0.53                      & 98\\% & 2.00 & \\Chart{1.00}{1.00}{0.90}{1.00}{1.00} &1 & 0 & 96.6 & 45.6 & 8.6\\% & 0.19                   & 96\\% & 8.00 & \\Chart{1.00}{1.00}{0.80}{1.00}{1.00} & 2 & 0 & 84.4 & 7.2 & 14.4\\% & 0.35 \\\\ \n",
    "# D-III & 0.41 & 0.16 & 0.64 & 0.20                     & 90\\% & 15.00 & \\Chart{1.00}{0.90}{0.90}{0.70}{1.00} &5 & 0 & 75.8 & 1392.2 & 15.2\\% & 0.51               & 54\\% & 213.00 & \\Chart{1.00}{0.90}{0.70}{0.10}{0.00} & 21 & 2 & 86.6 & 2543.7 & 29.6\\% & 3.92 \\\\ \n",
    "# D-IV & 0.66 & 0.01 & 0.67 & 0.64                      & 94\\% & 8.00 & \\Chart{1.00}{0.90}{1.00}{1.00}{0.80} &3 & 0 & 87.1 & 19.5 & 9.4\\% & 0.10                   & 60\\% & 255.00 & \\Chart{1.00}{0.90}{1.00}{0.10}{0.00} & 20 & 0 & 64.0 & 1496.5 & 18.6\\% & 0.74 \\\\ \n",
    "# E-II & 0.41 & 0.14 & 0.60 & 0.20                      & 46\\% & 168.00 & \\Chart{1.00}{0.20}{0.80}{0.20}{0.10} &15 & 12 & 123.7 & 918.6 & 15.0\\% & 1.01            & 70\\% & 45.00 & \\Chart{0.90}{0.90}{0.60}{0.70}{0.40} & 5 & 10 & 220.7 & 3958.7 & 24.6\\% & 0.71 \\\\ \n",
    "# E-III & 0.39 & 0.16 & 0.66 & 0.19                     & 64\\% & 58.00 & \\Chart{0.90}{0.90}{0.50}{0.40}{0.50} &18 & 0 & 74.4 & 342.8 & 11.4\\% & 0.16               & 20\\% & 120.00 & \\Chart{0.80}{0.00}{0.20}{0.00}{0.00} & 29 & 11 & 323.8 & 14913.8 & 22.2\\% & 0.41 \\\\ \n",
    "# F-I & 0.65 & 0.01 & 0.66 & 0.64                       & 66\\% & 108.00 & \\Chart{1.00}{1.00}{0.60}{0.40}{0.30} &14 & 3 & 90.3 & 210.5 & 11.8\\% & 0.04              & 58\\% & 122.00 & \\Chart{0.80}{0.80}{0.80}{0.50}{0.00} & 16 & 5 & 156.8 & 8826.7 & 16.0\\% & 0.14 \\\\ \n",
    "# F-II & 0.66 & 0.01 & 0.66 & 0.65                      & 80\\% & 65.00 & \\Chart{1.00}{1.00}{0.90}{0.70}{0.40} &5 & 5 & 108.2 & 264.1 & 8.2\\% & 0.24                & 92\\% & 7.00 & \\Chart{1.00}{1.00}{0.90}{0.90}{0.80} & 3 & 1 & 92.6 & 237.6 & 18.6\\% & 0.56 \\\\ \n",
    "# F-III & 0.66 & 0.00 & 0.66 & 0.65                     & 100\\% & 0.00 & \\Chart{1.00}{1.00}{1.00}{1.00}{1.00} &0 & 0 & 91.7 & 8.0 & 5.6\\% & 0.03                   & 100\\% & 0.00 & \\Chart{1.00}{1.00}{1.00}{1.00}{1.00} & 0 & 0 & 77.2 & 9.7 & 17.4\\% & 0.13 \\\\ \n",
    "# F-IV & 0.63 & 0.00 & 0.64 & 0.63                      & 100\\% & 0.00 & \\Chart{1.00}{1.00}{1.00}{1.00}{1.00} &0 & 0 & 85.3 & 0.5 & 7.2\\% & 0.02                   & 88\\% & 2.00 & \\Chart{0.80}{0.90}{0.90}{0.90}{0.90} & 6 & 0 & 90.0 & 2.5 & 19.4\\% & 0.01 \\\\ \n",
    "# F-V & 0.64 & 0.02 & 0.66 & 0.60                       & 66\\% & 33.00 & \\Chart{0.80}{0.90}{0.60}{0.50}{0.50} &15 & 2 & 99.8 & 175.3 & 14.6\\% & 0.30               & 84\\% & 43.00 & \\Chart{1.00}{0.90}{1.00}{0.80}{0.50} & 8 & 0 & 123.4 & 731.7 & 12.2\\% & 0.38 \\\\ \n",
    "# G-I & 0.50 & 0.09 & 0.58 & 0.34                       & 82\\% & 92.00 & \\Chart{0.30}{1.00}{1.00}{0.80}{1.00} &5 & 4 & 89.5 & 1336.1 & 12.4\\% & 0.83               & 24\\% & 153.00 & \\Chart{0.00}{0.90}{0.00}{0.00}{0.30} & 38 & 0 & 40.8 & 3230.7 & 14.4\\% & 0.57 \\\\ \n",
    "# G-II & 0.50 & 0.16 & 0.66 & 0.23                      & 46\\% & 213.00 & \\Chart{1.00}{0.90}{0.30}{0.10}{0.00} &21 & 6 & 49.0 & 986.5 & 15.2\\% & 0.68              & 56\\% & 113.00 & \\Chart{0.90}{0.80}{0.70}{0.20}{0.20} & 19 & 3 & 44.3 & 78.9 & 33.4\\% & 2.10 \\\\ \n",
    "# G-III & 0.65 & 0.01 & 0.66 & 0.64                     & 74\\% & 28.00 & \\Chart{1.00}{0.80}{0.70}{0.60}{0.60} &13 & 0 & 75.0 & 168.1 & 9.6\\% & 0.25                & 52\\% & 32.00 & \\Chart{0.70}{0.70}{0.50}{0.40}{0.30} & 6 & 18 & 197.5 & 2127.2 & 49.0\\% & 2.67 \\\\ \n",
    "# G-IV & 0.66 & 0.00 & 0.66 & 0.66                      & 92\\% & 2.00 & \\Chart{0.90}{0.90}{0.90}{1.00}{0.90} &4 & 0 & 92.2 & 32.5 & 8.0\\% & 0.05                   & 94\\% & 8.00 & \\Chart{1.00}{1.00}{1.00}{0.90}{0.80} & 0 & 3 & 91.9 & 559.3 & 29.6\\% & 3.32 \\\\ \n",
    "# G-V & 0.66 & 0.02 & 0.67 & 0.62                       & 88\\% & 17.00 & \\Chart{1.00}{1.00}{0.90}{0.80}{0.70} &6 & 0 & 85.5 & 75.8 & 11.8\\% & 0.47                 & 66\\% & 173.00 & \\Chart{1.00}{0.90}{0.90}{0.50}{0.00} & 13 & 4 & 143.8 & 3136.5 & 32.0\\% & 1.59 \\\\ \n",
    "# G-VI & 0.65 & 0.01 & 0.66 & 0.64                      & 46\\% & 258.00 & \\Chart{1.00}{1.00}{0.00}{0.30}{0.00} &0 & 27 & 131.5 & 932.6 & 6.2\\% & 0.49              & 68\\% & 72.00 & \\Chart{0.90}{0.90}{0.80}{0.50}{0.30} & 11 & 5 & 118.8 & 2413.4 & 22.4\\% & 1.09 \\\\ \n",
    "# G-VI & 0.64 & 0.03 & 0.66 & 0.60                      & 88\\% & 7.00 & \\Chart{0.90}{1.00}{0.80}{0.90}{0.80} &6 & 0 & 91.7 & 29.6 & 5.8\\% & 0.01                   & 52\\% & 187.00 & \\Chart{0.90}{0.80}{0.80}{0.10}{0.00} & 15 & 9 & 103.6 & 123.9 & 17.6\\% & 1.79 \\\\ \n",
    "# G-VII & 0.65 & 0.02 & 0.66 & 0.62                     & 86\\% & 28.00 & \\Chart{1.00}{1.00}{0.90}{0.80}{0.60} &7 & 0 & 78.8 & 297.7 & 14.6\\% & 1.61                & 82\\% & 32.00 & \\Chart{0.90}{0.90}{0.90}{0.90}{0.50} & 4 & 5 & 108.1 & 259.0 & 40.2\\% & 1.17 \\\\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfEnv4",
   "language": "python",
   "name": "tfenv4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
