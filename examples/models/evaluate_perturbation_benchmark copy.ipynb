{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Tuple, List, Dict, Any, Optional, Union\n",
    "import ijson\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findClosestPoint(\n",
    "    point: Tuple[float, float, float], pointsStr: str\n",
    ") -> Tuple[int, int]:\n",
    "    # point is a tuple of (x, y, z)\n",
    "    # pointsStr is a string of a list of points seperated by @\n",
    "    stringPoints = pointsStr.split(\"@\")\n",
    "    point = [float(value) for value in point]\n",
    "    # cast each strring from 0.1,0.2,0.3 to [0.1, 0.2, 0.3]\n",
    "    points = [list(map(float, point.split(\",\"))) for point in stringPoints]\n",
    "    # find closest match of point in points\n",
    "    closest = min(points, key=lambda p: sum((a - b) ** 2 for a, b in zip(p, point)))\n",
    "    # return index of closest point and total length of points\n",
    "    return (points.index(closest), len(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distanceToLastRoadPoint(\n",
    "        point: Tuple[float, float, float], pointsStr: str\n",
    ") -> float:\n",
    "    stringPoints = pointsStr.split(\"@\")\n",
    "    point = [float(value) for value in point]\n",
    "    points = [list(map(float, point.split(\",\"))) for point in stringPoints]\n",
    "    lastPoint = points[-1]\n",
    "    # find index in last point closest to 0.56\n",
    "    # round to 2 decomal places\n",
    "    if round(lastPoint[1], 1) == 0.5:\n",
    "        lastPoint[1], lastPoint[2] = lastPoint[2], lastPoint[1]\n",
    "\n",
    "    return sum((a - b) ** 2 for a, b in zip(lastPoint, point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road1 = []\n",
    "road2 = []\n",
    "road3 = []\n",
    "road4 = []\n",
    "road5 = []\n",
    "road6 = []\n",
    "road7 = []\n",
    "road8 = []\n",
    "road9 = []\n",
    "\n",
    "\n",
    "def loadJsons(filenames):\n",
    "    results: Dict[str, Dict[int, Any]] = {}\n",
    "    roads: Dict[str, Any] = {}\n",
    "    road_success_dict = {}\n",
    "\n",
    "    df_dict = {}\n",
    "    for filename in filenames:\n",
    "        # iterate over each object in the json file\n",
    "        with open(filename, \"r\") as f:\n",
    "            objects = ijson.items(f, \"item\")\n",
    "            for benchmarking_obj in objects:\n",
    "                # access the scenario\n",
    "                scenario = benchmarking_obj[\"scenario\"]\n",
    "                # acess the waypoints within scenario\n",
    "                waypoints = scenario[\"waypoints\"]\n",
    "                # get a hash of the waypoints\n",
    "                waypointsHash = hashlib.sha256(str(waypoints).encode()).hexdigest()\n",
    "                # if the hash equial asjdn write the object in a list\n",
    "                # if waypointsHash == \"44160852c628732f811ea9907657f37de4d02894da958e5d858d669ff2578995\":\n",
    "                #    road1.append(benchmarking_obj)\n",
    "                # elif waypointsHash == \"be859fbf2b9946c8d4834d026e413df51dc9f70bd637bcd0dc45ec7e8e3926af\":\n",
    "                #    road2.append(benchmarking_obj)\n",
    "                # elif waypointsHash == \"0a246a82022e55cbd10c2d46db0e90f41111798ccb7179b3c9ef1f9d7e549882\":\n",
    "                #    road3.append(benchmarking_obj)\n",
    "                # elif waypointsHash == \"493f3da92ee367c09bf8647aab2fb671462701ef0397c3749dd93b18585e1aa1\":\n",
    "                #    road4.append(benchmarking_obj)\n",
    "                # elif waypointsHash == \"382e50f80e16e79861d84d5d102dae4b793c449941b48d4963560d13e4b0f053\":\n",
    "                #    road5.append(benchmarking_obj)\n",
    "                # elif waypointsHash == \"90a8e61085e4e9918cb2c35f1d751b76a3a4b9ba5ef677deaa441487827fd026\":\n",
    "                #    road8.append(benchmarking_obj)\n",
    "\n",
    "                roads[waypointsHash] = waypoints\n",
    "\n",
    "                # get the perturbation_function\n",
    "                perturbation_function = scenario[\"perturbation_function\"]\n",
    "                # get the perturbation_scale\n",
    "                perturbation_scale = scenario[\"perturbation_scale\"]\n",
    "                # get the boolean value for isSuccess\n",
    "                isSuccess = benchmarking_obj[\"isSuccess\"]\n",
    "\n",
    "                # get the average value of the float list in xte\n",
    "                xte = benchmarking_obj[\"xte\"]\n",
    "                xteAvg = sum(xte) / len(xte)\n",
    "                xteAvg = float(xteAvg)\n",
    "                max_xte = max(xte)\n",
    "                max_xte = float(max_xte)\n",
    "\n",
    "                # get the last value in the pos list\n",
    "                pos = benchmarking_obj[\"pos\"]\n",
    "                posLast = pos[-1]\n",
    "                # swap the y and z values\n",
    "                posLast[1], posLast[2] = posLast[2], posLast[1]\n",
    "\n",
    "                # find the closest point to the last value in pos\n",
    "                closestPoint, totalPoints = findClosestPoint(posLast, waypoints)\n",
    "\n",
    "                quickness = 1 - (closestPoint / totalPoints)\n",
    "\n",
    "                # get distance to last road point\n",
    "                distance = distanceToLastRoadPoint(posLast, waypoints)\n",
    "\n",
    "                isSuccess = max_xte < 2.0 and distance <= 6.0\n",
    "\n",
    "                # check if there is a key for the perturbation_function\n",
    "                if perturbation_function not in results:\n",
    "                    results[perturbation_function] = {}\n",
    "                # check if there is a key for the perturbation_scale\n",
    "                if perturbation_scale not in results[perturbation_function]:\n",
    "                    results[perturbation_function][perturbation_scale] = {\n",
    "                        \"success\": [],\n",
    "                        \"xte\": [],\n",
    "                        \"quickness\": [],\n",
    "                        \"roadHash\": [],\n",
    "                    }\n",
    "                # increment the success or failure\n",
    "                results[perturbation_function][perturbation_scale][\"success\"].append(\n",
    "                    isSuccess\n",
    "                )\n",
    "\n",
    "                # append the xte and quickness values\n",
    "                results[perturbation_function][perturbation_scale][\"xte\"].append(xteAvg)\n",
    "                results[perturbation_function][perturbation_scale][\"quickness\"].append(\n",
    "                    quickness\n",
    "                )\n",
    "                results[perturbation_function][perturbation_scale][\"roadHash\"].append(\n",
    "                    waypointsHash\n",
    "                )\n",
    "\n",
    "                # other index\n",
    "                index = f\"Scale_{perturbation_scale}_Success\"\n",
    "                index2 = f\"Scale_{perturbation_scale}_XTE\"\n",
    "                index3 = f\"Scale_{perturbation_scale}_Quickness\"\n",
    "                index4 = f\"Scale_{perturbation_scale}_RoadHash\"\n",
    "                index5 = f\"Scale_{perturbation_scale}_MaxXTE\"\n",
    "                index6 = f\"Scale_{perturbation_scale}_Distance\"\n",
    "\n",
    "                if perturbation_function not in df_dict:\n",
    "                    df_dict[perturbation_function] = {}\n",
    "                if index not in df_dict[perturbation_function]:\n",
    "                    df_dict[perturbation_function][index] = [isSuccess]\n",
    "                else:\n",
    "                    df_dict[perturbation_function][index].append(isSuccess)\n",
    "                if index2 not in df_dict[perturbation_function]:\n",
    "                    df_dict[perturbation_function][index2] = [xteAvg]\n",
    "                else:\n",
    "                    df_dict[perturbation_function][index2].append(xteAvg)\n",
    "                if index3 not in df_dict[perturbation_function]:\n",
    "                    df_dict[perturbation_function][index3] = [quickness]\n",
    "                else:\n",
    "                    df_dict[perturbation_function][index3].append(quickness)\n",
    "                if index4 not in df_dict[perturbation_function]:\n",
    "                    df_dict[perturbation_function][index4] = [waypointsHash]\n",
    "                else:\n",
    "                    df_dict[perturbation_function][index4].append(waypointsHash)\n",
    "                if index5 not in df_dict[perturbation_function]:\n",
    "                    df_dict[perturbation_function][index5] = [max_xte]\n",
    "                else:\n",
    "                    df_dict[perturbation_function][index5].append(max_xte)\n",
    "                if index6 not in df_dict[perturbation_function]:\n",
    "                    df_dict[perturbation_function][index6] = [distance]\n",
    "                else:\n",
    "                    df_dict[perturbation_function][index6].append(distance)\n",
    "\n",
    "                # check if there is a key for roadHash in roads\n",
    "                if waypointsHash not in roads:\n",
    "                    roads[waypointsHash] = waypoints\n",
    "                if waypointsHash not in road_success_dict:\n",
    "                    road_success_dict[\n",
    "                        waypointsHash\n",
    "                    ] = f\"{perturbation_function}_{perturbation_scale}-\"\n",
    "                if isSuccess:\n",
    "                    road_success_dict[\n",
    "                        waypointsHash\n",
    "                    ] += f\"{perturbation_function}_{perturbation_scale}-\"\n",
    "\n",
    "    return results, roads, road_success_dict, df_dict\n",
    "\n",
    "\n",
    "results, roads, road_success_dict, df_dict = loadJsons([\"./data.json\"])\n",
    "\n",
    "if False:\n",
    "    # write all items from road1 in a json file\n",
    "    with open(\"donkey_benchmark_normal_perturbations_road1_dave.json\", \"w\") as f:\n",
    "        json.dump(road1, f, indent=4)\n",
    "    # do tihs for all roads\n",
    "    with open(\"donkey_benchmark_normal_perturbations_road2_dave.json\", \"w\") as f:\n",
    "        json.dump(road2, f, indent=4)\n",
    "    with open(\"donkey_benchmark_normal_perturbations_road3_dave.json\", \"w\") as f:\n",
    "        json.dump(road3, f, indent=4)\n",
    "    with open(\"donkey_benchmark_normal_perturbations_road4_dave.json\", \"w\") as f:\n",
    "        json.dump(road4, f, indent=4)\n",
    "    with open(\"donkey_benchmark_normal_perturbations_road5_dave.json\", \"w\") as f:\n",
    "        json.dump(road5, f, indent=4)\n",
    "    with open(\"donkey_benchmark_normal_perturbations_road8_dave.json\", \"w\") as f:\n",
    "        json.dump(road8, f, indent=4)\n",
    "\n",
    "    # create a zip from all road jsons\n",
    "    # zip all the files\n",
    "    import zipfile\n",
    "\n",
    "    with zipfile.ZipFile(\n",
    "        \"donkey_benchmark_normal_perturbations_dave.zip\", \"w\", zipfile.ZIP_DEFLATED\n",
    "    ) as zipf:\n",
    "        zipf.write(\"donkey_benchmark_normal_perturbations_road1_dave.json\")\n",
    "        zipf.write(\"donkey_benchmark_normal_perturbations_road2_dave.json\")\n",
    "        zipf.write(\"donkey_benchmark_normal_perturbations_road3_dave.json\")\n",
    "        zipf.write(\"donkey_benchmark_normal_perturbations_road4_dave.json\")\n",
    "        zipf.write(\"donkey_benchmark_normal_perturbations_road5_dave.json\")\n",
    "        zipf.write(\"donkey_benchmark_normal_perturbations_road8_dave.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# iterate over a dict\n",
    "for key, value in results.items():\n",
    "    # iterate over a dict\n",
    "    for key2, value2 in value.items():\n",
    "        # iterate over a dict\n",
    "        for key3, value3 in value2.items():\n",
    "            # print the key and value\n",
    "            print(key, key2, key3, value3)\n",
    "\n",
    "\n",
    "def delete_entries(json_file, field, values):\n",
    "    # Read the JSON file\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Delete the first and third entry from the list\n",
    "    data = data[1:3] + data[4:]\n",
    "\n",
    "    # Write the updated JSON data back to the file\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "def findEntries(json_file, values):\n",
    "    # Read the JSON file\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # iterate over the data\n",
    "    counter = 0\n",
    "    for entry in data:\n",
    "        counter += 1\n",
    "        # check if the entry has the field\n",
    "        if \"scenario\" in entry and entry[\"scenario\"][\"waypoints\"] in values:\n",
    "            print(\"counter has value in field\")\n",
    "\n",
    "    # Write the updated JSON data back to the file\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def findEntries(json_file, values):\n",
    "    # Read the JSON file\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # iterate over the data\n",
    "    counter = 0\n",
    "    for entry in data:\n",
    "        counter += 1\n",
    "        # check if the entry has the field\n",
    "        if \"scenario\" in entry and entry[\"scenario\"][\"waypoints\"] in values:\n",
    "            print(\"counter has value in field\")\n",
    "\n",
    "    # Write the updated JSON data back to the file\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from plottable import ColumnDefinition, Table\n",
    "from plottable.cmap import normed_cmap\n",
    "from plottable.formatters import decimal_to_percent\n",
    "from plottable.plots import circled_image # image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(df_dict, orient=\"index\").round(2)\n",
    "\n",
    "# copy dataframe as backup\n",
    "df_copy = df.copy()\n",
    "\n",
    "scale_0_columns = [col for col in df.columns if \"Scale_0\" in col and not \"RoadHash\" in col]\n",
    "scale_1_columns = [col for col in df.columns if \"Scale_1\" in col and not \"RoadHash\" in col]\n",
    "scale_2_columns = [col for col in df.columns if \"Scale_2\" in col and not \"RoadHash\" in col]\n",
    "scale_3_columns = [col for col in df.columns if \"Scale_3\" in col and not \"RoadHash\" in col]\n",
    "scale_4_columns = [col for col in df.columns if \"Scale_4\" in col and not \"RoadHash\" in col]\n",
    "\n",
    "\n",
    "df.drop(\n",
    "    [\n",
    "        \"Scale_0_RoadHash\",\n",
    "        \"Scale_1_RoadHash\",\n",
    "        \"Scale_2_RoadHash\",\n",
    "        \"Scale_3_RoadHash\",\n",
    "        \"Scale_4_RoadHash\",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Scale_0_XTE\"] = df[\"Scale_0_XTE\"].apply(lambda x: sum(x) / len(x))\n",
    "# format to 2 decimal places\n",
    "df[\"Scale_0_XTE\"] = df[\"Scale_0_XTE\"].map(\"{:.2f}\".format)\n",
    "df[\"Scale_1_XTE\"] = df[\"Scale_1_XTE\"].apply(lambda x: sum(x) / len(x))\n",
    "df[\"Scale_1_XTE\"] = df[\"Scale_1_XTE\"].map(\"{:.2f}\".format)\n",
    "df[\"Scale_2_XTE\"] = df[\"Scale_2_XTE\"].apply(lambda x: sum(x) / len(x))\n",
    "df[\"Scale_2_XTE\"] = df[\"Scale_2_XTE\"].map(\"{:.2f}\".format)\n",
    "df[\"Scale_3_XTE\"] = df[\"Scale_3_XTE\"].apply(lambda x: sum(x) / len(x))\n",
    "df[\"Scale_3_XTE\"] = df[\"Scale_3_XTE\"].map(\"{:.2f}\".format)\n",
    "df[\"Scale_4_XTE\"] = df[\"Scale_4_XTE\"].apply(lambda x: sum(x) / len(x))\n",
    "df[\"Scale_4_XTE\"] = df[\"Scale_4_XTE\"].map(\"{:.2f}\".format)\n",
    "\n",
    "# do the same for quickness\n",
    "df[\"Scale_0_Quickness\"] = df[\"Scale_0_Quickness\"].apply(lambda x: sum(x) / len(x))\n",
    "# format to 2 decimal places\n",
    "df[\"Scale_0_Quickness\"] = df[\"Scale_0_Quickness\"].map(\"{:.2f}\".format)\n",
    "df[\"Scale_1_Quickness\"] = df[\"Scale_1_Quickness\"].apply(lambda x: sum(x) / len(x))\n",
    "df[\"Scale_1_Quickness\"] = df[\"Scale_1_Quickness\"].map(\"{:.2f}\".format)\n",
    "df[\"Scale_2_Quickness\"] = df[\"Scale_2_Quickness\"].apply(lambda x: sum(x) / len(x))\n",
    "df[\"Scale_2_Quickness\"] = df[\"Scale_2_Quickness\"].map(\"{:.2f}\".format)\n",
    "df[\"Scale_3_Quickness\"] = df[\"Scale_3_Quickness\"].apply(lambda x: sum(x) / len(x))\n",
    "df[\"Scale_3_Quickness\"] = df[\"Scale_3_Quickness\"].map(\"{:.2f}\".format)\n",
    "df[\"Scale_4_Quickness\"] = df[\"Scale_4_Quickness\"].apply(lambda x: sum(x) / len(x))\n",
    "df[\"Scale_4_Quickness\"] = df[\"Scale_4_Quickness\"].map(\"{:.2f}\".format)\n",
    "\n",
    "\n",
    "# within each success column, count the amount of True values and divide by the total amount of values\n",
    "df[\"Scale_0_Success\"] = df[\"Scale_0_Success\"].apply(lambda x: sum(x)/len(x))\n",
    "df[\"Scale_1_Success\"] = df[\"Scale_1_Success\"].apply(lambda x: sum(x)/len(x))\n",
    "df[\"Scale_2_Success\"] = df[\"Scale_2_Success\"].apply(lambda x: sum(x)/len(x))\n",
    "df[\"Scale_3_Success\"] = df[\"Scale_3_Success\"].apply(lambda x: sum(x)/len(x))\n",
    "df[\"Scale_4_Success\"] = df[\"Scale_4_Success\"].apply(lambda x: sum(x)/len(x))\n",
    "# format to % with no decimal places\n",
    "\n",
    "df = df.round(2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = LinearSegmentedColormap.from_list(\n",
    "    name=\"bugw\", colors=[\"#ff1100\", \"#d95d55\", \"#c9ecb4\", \"#93d3ab\", \"#1a8a23\"], N=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_defs = (\n",
    "    [\n",
    "        ColumnDefinition(\n",
    "            name=col,\n",
    "            title=col.replace(\"Scale_0_\", \"\"),\n",
    "            cmap=cmap,\n",
    "            group=\"Intensity 0\",\n",
    "        )\n",
    "        for col in scale_0_columns\n",
    "\n",
    "    ]\n",
    "    + [\n",
    "        ColumnDefinition(\n",
    "            name=col,\n",
    "            title=col.replace(\"Scale_1_\", \"\"),\n",
    "            cmap=cmap,\n",
    "            group=\"Intensity 1\",\n",
    "        )\n",
    "                for col in scale_1_columns\n",
    "\n",
    "    ]\n",
    "    + [\n",
    "        ColumnDefinition(\n",
    "            name=col,\n",
    "            title=col.replace(\"Scale_2_\", \"\"),\n",
    "            cmap=cmap,\n",
    "            group=\"Intensity 2\",\n",
    "        )\n",
    "                for col in scale_2_columns\n",
    "\n",
    "    ]\n",
    "    + [\n",
    "        ColumnDefinition(\n",
    "            name=col,\n",
    "            title=col.replace(\"Scale_3_\", \"\"),\n",
    "            cmap=cmap,\n",
    "            group=\"Intensity 3\",\n",
    "        )\n",
    "                for col in scale_3_columns\n",
    "\n",
    "    ]\n",
    "    + [\n",
    "        ColumnDefinition(\n",
    "            name=col,\n",
    "            title=col.replace(\"Scale_4_\", \"\"),\n",
    "            cmap=cmap,\n",
    "            group=\"Intensity 4\",\n",
    "        )\n",
    "                for col in scale_4_columns\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25, 22))\n",
    "\n",
    "table = Table(\n",
    "    df,\n",
    "    column_definitions=col_defs,\n",
    "    row_dividers=True,\n",
    "    footer_divider=True,\n",
    "    ax=ax,\n",
    "    textprops={\"fontsize\": 14},\n",
    "    row_divider_kw={\"linewidth\": 1, \"linestyle\": (0, (1, 5))},\n",
    "    col_label_divider_kw={\"linewidth\": 1, \"linestyle\": \"-\"},\n",
    "    column_border_kw={\"linewidth\": 1, \"linestyle\": \"-\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_heatmap(df, kpi, cmap='coolwarm', fmt='.2f'):\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df = df.fillna(0)  # Fill NaN values with 0\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(df.T, annot=True, cmap=cmap, fmt=fmt)\n",
    "    plt.title(f'Heatmap for {kpi}')\n",
    "    plt.xlabel('Perturbation')\n",
    "    plt.ylabel('Intensity Scale')\n",
    "    # change font size of whole heatmap to 10\n",
    "    plt.rcParams['font.size'] = 10\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_kpi_df(kpi,df):\n",
    "\n",
    "    df_copy = df.reset_index()\n",
    "    scales = sorted(set(col.split('_')[1] for col in df_copy.columns if 'Scale' in col))\n",
    "    perturbations = df_copy['index'].tolist()\n",
    "    # Initialize an empty dataframe\n",
    "    kpi_df = pd.DataFrame(index=scales, columns=perturbations)\n",
    "    # Fill the dataframe with values\n",
    "    for scale in scales:\n",
    "        for perturbation in perturbations:\n",
    "            # Construct the column name\n",
    "            col_name = f'Scale_{scale}_{kpi}'\n",
    "            if col_name in df_copy.columns:\n",
    "                kpi_df.at[scale, perturbation] = df_copy.loc[df_copy['index'] == perturbation, col_name].values[0]\n",
    "            else:\n",
    "                # Assign 0 if the perturbation does not exist for the scale\n",
    "                kpi_df.at[scale, perturbation] = 0\n",
    "\n",
    "    # Convert index to integer for proper sorting\n",
    "    kpi_df.index = kpi_df.index.astype(int)\n",
    "    kpi_df.sort_index(inplace=True)\n",
    "\n",
    "    return kpi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the perturbations in the df based on the success rate over all scales\n",
    "df_copy = df.copy()\n",
    "df_copy[\"Avgerage_Success\"] = df_copy.filter(regex=\"Success\").mean(axis=1).tolist()\n",
    "df_copy = df_copy.sort_values(\n",
    "    by=[\n",
    "        \"Avgerage_Success\",\n",
    "        \"Scale_0_Success\",\n",
    "        \"Scale_1_Success\",\n",
    "        \"Scale_2_Success\",\n",
    "        \"Scale_3_Success\",\n",
    "        \"Scale_4_Success\",\n",
    "    ],\n",
    "    ascending=False,\n",
    ")\n",
    "\n",
    "for i in range(4):\n",
    "    start = i * 11\n",
    "    # take the first 10 entries\n",
    "    print(f\"from {start}\")\n",
    "    df_copy_i = df_copy[start : start + 11]\n",
    "\n",
    "    # get the perturbations\n",
    "    perturbations = df_copy_i.index.tolist()\n",
    "    # create a dict of the scales and all success rates in this scale\n",
    "    success = {\n",
    "        \"Intensity 0\": df_copy_i[\"Scale_0_Success\"].tolist(),\n",
    "        \"Intensity 1\": df_copy_i[\"Scale_1_Success\"].tolist(),\n",
    "        \"Intensity 2\": df_copy_i[\"Scale_2_Success\"].tolist(),\n",
    "        \"Intensity 3\": df_copy_i[\"Scale_3_Success\"].tolist(),\n",
    "        \"Intensity 4\": df_copy_i[\"Scale_4_Success\"].tolist(),\n",
    "    }\n",
    "\n",
    "    # get min and max of all success rates\n",
    "    min_success = min(df_copy_i.filter(regex=\"Success\").min(axis=1).tolist())\n",
    "    max_success = max(df_copy_i.filter(regex=\"Success\").max(axis=1).tolist())\n",
    "\n",
    "    x = np.arange(len(perturbations))  # the label locations\n",
    "    width = 0.15  # the width of the bars\n",
    "    multiplier = 0\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(25, 10))\n",
    "\n",
    "    for attribute, success in success.items():\n",
    "        offset = width * multiplier\n",
    "\n",
    "        # Create a colormap\n",
    "        cmap = LinearSegmentedColormap.from_list(\n",
    "            name=\"bugw\",\n",
    "            colors=[\"#ff1100\", \"#d95d55\", \"#c9ecb4\", \"#93d3ab\", \"#1a8a23\"],\n",
    "            N=256,\n",
    "        )\n",
    "        # Normalize the success values to the range [0, 1]\n",
    "        norm = plt.Normalize(0, 1)\n",
    "\n",
    "        # Calculate the colors\n",
    "        colors = cmap(norm(success))\n",
    "\n",
    "        rects1 = ax.bar(\n",
    "            x + offset,\n",
    "            success,\n",
    "            width,\n",
    "            label=attribute,\n",
    "            color=colors,\n",
    "        )\n",
    "        ax.bar_label(rects1, padding=3, fmt=\"%.2f %%\", rotation=90)\n",
    "\n",
    "        multiplier += 1\n",
    "\n",
    "    # get the average value for each perturbation and filter for success\n",
    "    average = df_copy_i.filter(regex=\"Success\").mean(axis=1).tolist()\n",
    "\n",
    "    for i in range(len(perturbations)):\n",
    "        ax.plot(\n",
    "            [i - width, i + 5 * width],\n",
    "            [average[i], average[i]],\n",
    "            color=\"r\",\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel(\"Success Rate\")\n",
    "    ax.set_title(\"Success Rate per Perturbation\")\n",
    "    ax.set_xticks(x + width, perturbations)\n",
    "    ax.legend(\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, -0.05),\n",
    "        fancybox=True,\n",
    "        shadow=True,\n",
    "        ncol=5,\n",
    "    )\n",
    "\n",
    "    # set y range from min to max\n",
    "    ax.set_ylim([min_success - 0.05, max_success + 0.05])\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
